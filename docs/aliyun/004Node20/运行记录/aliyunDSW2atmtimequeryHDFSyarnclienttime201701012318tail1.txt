-rwxr-xr-x  1 hadoop hadoop        167 Dec 31 15:01 getdataref.sh
-rwxr-xr-x  1 hadoop hadoop        163 Dec 31 15:01 getdata.sh
-rwxr-xr-x  1 hadoop hadoop        163 Dec 30 19:27 get.sh
drwxrwxr-x  2 hadoop hadoop       4096 Dec 30 17:49 lib
-rwxr-xr-x  1 hadoop hadoop       1329 Dec 30 18:14 ossforprestorestart.sh
-rwxr-xr-x  1 hadoop hadoop        184 Dec 30 18:19 putrun.sh
-rwxr-xr-x  1 hadoop hadoop        163 Dec 30 18:21 put.sh
drwxrwxr-x  2 hadoop hadoop       4096 Dec 31 16:35 query
drwxrwxr-x  2 hadoop hadoop       4096 Jan  1 03:39 queryD
-rwxr-xr-x  1 hadoop hadoop         72 Dec 30 16:27 realpath.sh
-rwxr-xr-x  1 hadoop hadoop        101 Jan  1 16:51 refreshXubo.sh
-rwxr-xr-x  1 hadoop hadoop       1274 Dec 30 18:29 run.sh
drwxr-xr-x 15 hadoop hadoop       4096 Dec 31 15:47 spark-1.5.2
-rw-r--r--  1 hadoop hadoop       4149 Jan 27  2016 SparkPi.jar
-rwxr-xr-x  1 hadoop hadoop        235 Dec 30 18:56 submitJob.sh
drwxrwxr-x  3 hadoop hadoop       4096 Dec 30 18:50 test
-rwxr-xr-x  1 hadoop hadoop      13269 Dec  2 15:58 test_isa
-rwxr-xr-x  1 hadoop hadoop         52 Dec 30 18:47 test.sh
drwxrwxr-x  3 hadoop hadoop       4096 Dec 30 17:49 tools
drwxrwxr-x  3 hadoop hadoop       4096 Jan  1 21:53 xubo
[hadoop@emr-header-1 ~]$ du -sh *
4.0K	addNode.sh
4.0K	aliyunDSW2atmtimequeryHDFSyarnclient.sh
4.0K	aliyunDSW2timequeryHDFS.sh
4.0K	aliyunDSW2timequeryHDFSyarnclient.sh
326M	backup
12K	bin
4.0K	dispatch.sh
32M	DL1Line.fasta
11M	DL1Line.fasta.gz
4.0G	DL8Line.fasta
2.3G	DL8Line.fasta.gz
8.1G	DL9Line.fasta
4.5G	DL9Line.fasta.gz
2.3M	DSA.jar
4.0K	getdataref.sh
4.0K	getdata.sh
4.0K	get.sh
516K	lib
4.0K	ossforprestorestart.sh
4.0K	putrun.sh
4.0K	put.sh
80K	query
644K	queryD
4.0K	realpath.sh
4.0K	refreshXubo.sh
4.0K	run.sh
387M	spark-1.5.2
8.0K	SparkPi.jar
4.0K	submitJob.sh
8.0K	test
16K	test_isa
4.0K	test.sh
196M	tools
248K	xubo
[hadoop@emr-header-1 ~]$ mkdir bigdataBackup
[hadoop@emr-header-1 ~]$ mv *.gz bi
bigdataBackup/ bin/           
[hadoop@emr-header-1 ~]$ mv *.gz bigdataBackup/
[hadoop@emr-header-1 ~]$ ls
addNode.sh                               aliyunDSW2timequeryHDFS.sh            backup         bin          DL1Line.fasta  DL9Line.fasta  getdataref.sh  get.sh  ossforprestorestart.sh  put.sh  queryD       refreshXubo.sh  spark-1.5.2  submitJob.sh  test_isa  tools
aliyunDSW2atmtimequeryHDFSyarnclient.sh  aliyunDSW2timequeryHDFSyarnclient.sh  bigdataBackup  dispatch.sh  DL8Line.fasta  DSA.jar        getdata.sh     lib     putrun.sh               query   realpath.sh  run.sh          SparkPi.jar  test          test.sh   xubo
[hadoop@emr-header-1 ~]$ du -sh *
4.0K	addNode.sh
4.0K	aliyunDSW2atmtimequeryHDFSyarnclient.sh
4.0K	aliyunDSW2timequeryHDFS.sh
4.0K	aliyunDSW2timequeryHDFSyarnclient.sh
326M	backup
6.8G	bigdataBackup
12K	bin
4.0K	dispatch.sh
32M	DL1Line.fasta
4.0G	DL8Line.fasta
8.1G	DL9Line.fasta
2.3M	DSA.jar
4.0K	getdataref.sh
4.0K	getdata.sh
4.0K	get.sh
516K	lib
4.0K	ossforprestorestart.sh
4.0K	putrun.sh
4.0K	put.sh
80K	query
644K	queryD
4.0K	realpath.sh
4.0K	refreshXubo.sh
4.0K	run.sh
387M	spark-1.5.2
8.0K	SparkPi.jar
4.0K	submitJob.sh
8.0K	test
16K	test_isa
4.0K	test.sh
196M	tools
248K	xubo
[hadoop@emr-header-1 ~]$ tar -zcvf backup201701012214.tar.gz backup/
backup/
backup/xubo201612312030.tar.gz
backup/txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219tail.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011354D9L1024N4L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011410D9L1024N4L10240N4M10G.txt
backup/query.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010902D9L1024N4L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail3.txt
backup/error201701011246.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011225D9L1024N4L10240N4M10Gtail1.txt
backup/txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302214.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011512L10240N4M10Gtail2xubo.txt
backup/xubo201701011322.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail4.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345tail.txt
backup/xubo20170101150[C2.tar.gz
backup/spark-1.5.2Master.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011512L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312139D91024N1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail2.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011225D9L1024N4L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010347D9L1024N4M10Gtail3.txt
backup/error201701011146.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010904D9L1024N4L10240N4M10Gtail2.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010047D9L392N40M10Gtail1.txt
backup/xubo201701012153.tar.gz
backup/xubo201701012040.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail3.txt
backup/xubo201612312154.tar.gz
backup/xubo201612302345.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010347D9L1024N4M10Gtail.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail6xubo.txt
backup/cpuinfo.txt
backup/txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219.txt
backup/xubo201612302112.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010347D9L1024N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail6.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010904D9L1024N4L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011416D9L1024N4L10240N4M10Gtail.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010345D9L1024N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312243D9L392N40M10Gtail1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010347D9L1024N4M10Gtail2.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612302523,txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011511L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345.txt
backup/xubo/
backup/xubo/project/
backup/xubo/project/SparkSW/
backup/xubo/project/SparkSW/output/
backup/xubo/project/SparkSW/output/time/
backup/xubo/project/SparkSW/output/time/20161231233636530DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20161231233636530DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20161231233636530DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101070154334DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101070154334DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101070154334DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20161231224342922DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20161231224342922DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20161231224342922DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101034814834DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101034814834DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101034814834DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101052520154DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101052520154DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101052520154DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101034349152DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101034349152DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101034349152DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101034306370DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101034306370DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101034306370DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101090428184DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101090428184DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101090428184DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101014756978DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101014756978DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101014756978DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101122553924DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101122553924DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101122553924DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101141637673DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101141637673DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101141637673DSW2ATM_queryFile_D9L10240N4_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101034423563DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101034423563DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101034423563DSW2ATM_queryFile_D9L10240N1_dbFile_D9L10240N4_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/xubo/project/SparkSW/output/time/20170101004706714DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/
backup/xubo/project/SparkSW/output/time/20170101004706714DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/part-00000
backup/xubo/project/SparkSW/output/time/20170101004706714DSW2ATM_queryFile_D9L392N40_dbFile_DL9Line.fasta_splitNum_256_taskNum_1_topK_5/_SUCCESS
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail2.txt
backup/v1.1.1.tar.gz
backup/xubo201612302232.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010047D9L392N40M10G.txt
backup/xubo201701010846.tar.gz
backup/xubo201701010100.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011512L10240N4M10Gtail2heaher1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail4xubo.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40M12G.txt
backup/error201701010046.txt
backup/xubo201701011939.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail1.txt
backup/xubo201701012026.tar.gz
backup/xubo20161230/
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312143D9L392N100.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312218D9L392N40M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011654node15.txt
backup/xubo201612311525.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail5.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011512L10240N4M10Gtail1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail4.txt
backup/xubo201701010830.tar.gz
backup/txtaliyunDSW2timequeryHDFSyarnclientD8yarn201612302211.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15.txt
backup/xubo201701011938.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011225D9L1024N4L10240N4M10Gtail2.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312243D9L392N40M10Gtail2.txt
backup/xubo201701011936.tar.gz
backup/xubo201701011644.tar.gz
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312243D9L392N40M10Gtail21.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010904D9L1024N4L10240N4M10Gtail1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201612312243D9L392N40M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail5.txt
backup/1.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010852D9L1024N4L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701011416D9L1024N4L10240N4M10G.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011758node20tail2xubo.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclienttime201701010347D9L1024N4M10Gtail1xubo.txt
backup/systemClose201701010855.txt
backup/aliyunDSW2atmtimequeryHDFSyarnclientL10240N4L392N40time201701011659node15tail1xubo.txt
[hadoop@emr-header-1 ~]$ 
[hadoop@emr-header-1 ~]$ 
[hadoop@emr-header-1 ~]$ du -sh *
4.0K	addNode.sh
4.0K	aliyunDSW2atmtimequeryHDFSyarnclient.sh
4.0K	aliyunDSW2timequeryHDFS.sh
4.0K	aliyunDSW2timequeryHDFSyarnclient.sh
326M	backup
311M	backup201701012214.tar.gz
6.8G	bigdataBackup
12K	bin
4.0K	dispatch.sh
32M	DL1Line.fasta
4.0G	DL8Line.fasta
8.1G	DL9Line.fasta
2.3M	DSA.jar
4.0K	getdataref.sh
4.0K	getdata.sh
4.0K	get.sh
516K	lib
4.0K	ossforprestorestart.sh
4.0K	putrun.sh
4.0K	put.sh
80K	query
644K	queryD
4.0K	realpath.sh
4.0K	refreshXubo.sh
4.0K	run.sh
387M	spark-1.5.2
8.0K	SparkPi.jar
4.0K	submitJob.sh
8.0K	test
16K	test_isa
4.0K	test.sh
196M	tools
248K	xubo
[hadoop@emr-header-1 ~]$ w
 22:21:27 up 2 days,  6:36, 13 users,  load average: 0.00, 0.01, 0.00
USER     TTY      FROM              LOGIN@   IDLE   JCPU   PCPU WHAT
root     pts/0    58.210.35.226    20:09   25:36   0.04s  0.04s bash
root     pts/1    58.210.35.226    21:50   30:56   0.00s  0.00s -bash
root     pts/2    58.210.35.226    21:50   30:52   0.00s  0.00s -bash
root     pts/3    58.210.35.226    21:50   30:49   0.00s  0.00s -bash
root     pts/4    58.210.35.226    21:50   30:46   0.00s  0.00s -bash
root     pts/5    58.210.35.226    12:41   30.00s  0.04s  0.00s sz backup201701012214.tar.gz
root     pts/6    58.210.35.226    21:50   30:42   0.00s  0.00s -bash
root     pts/7    58.210.35.226    19:43    1:42m  0.01s  0.01s bash
root     pts/8    58.210.35.226    21:58    6.00s  0.01s  0.00s scp backup201701012214.tar.gz xubo@219.219.220.222:~/
root     pts/9    123.117.85.242   22:17    8.00s  1.16s  1.15s less /usr/local/emr/emr-agent/logs/agent_10-27-71-161_2016-12-30_15-45-15.log
root     pts/11   58.210.35.226    00:46   28:23   0.21s  0.21s bash
root     pts/12   58.210.35.226    00:46   25:49   0.27s  0.27s bash
root     pts/15   58.210.35.226    00:46    0.00s  0.21s  0.00s w
[hadoop@emr-header-1 ~]$ ping 10.27.71.86
PING 10.27.71.86 (10.27.71.86) 56(84) bytes of data.
64 bytes from 10.27.71.86: icmp_seq=1 ttl=64 time=1.42 ms
64 bytes from 10.27.71.86: icmp_seq=2 ttl=64 time=0.182 ms
^C
--- 10.27.71.86 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1715ms
rtt min/avg/max/mdev = 0.182/0.801/1.420/0.619 ms
[hadoop@emr-header-1 ~]$ ssh 10.27.71.86
The authenticity of host '10.27.71.86 (10.27.71.86)' can't be established.
RSA key fingerprint is 4f:de:0e:31:98:95:9e:76:3d:b4:f8:1a:9b:6a:a1:5a.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '10.27.71.86' (RSA) to the list of known hosts.

Welcome to aliyun Elastic Compute Service!

[hadoop@emr-worker-20 ~]$ ls
[hadoop@emr-worker-20 ~]$ ls
[hadoop@emr-worker-20 ~]$ cat /etc/hosts
127.0.0.1 localhost
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.46.79.84 iZ23vt9ykohZ
10.46.71.22 iZ23o7qodjnZ
10.25.6.165 iZ23j3z85poZ
10.25.113.77 iZ25h5j1pblZ
10.25.197.239 iZ2582bjjvsZ
10.25.233.107 iZ23fcl3kmtZ
10.25.84.144 iZ239jrw8q2Z
10.26.248.27 iZ23xqt8umoZ
10.28.108.176 iZbp1ielabd9elwzi6s9vjZ
# todo delete when meta service ready
1.1.1.1   emr.pre.ap-southeast-1.aliyuncs.com
1.1.1.1   emr.pre.cn-shenzhen.aliyuncs.com
1.1.1.1   emr.pre.cn-qingdao.aliyuncs.com
1.1.1.1   emr.pre.cn-beijing.aliyuncs.com
1.1.1.1   emr.pre.cn-shanghai.aliyuncs.com
1.1.1.1   emr.pre.cn-hongkong.aliyuncs.com
1.1.1.1   emr.pre.us-west-1.aliyuncs.com

1.1.1.1   emr-as.pre.ap-southeast-1.aliyuncs.com
1.1.1.1   emr-as.pre.cn-shenzhen.aliyuncs.com
1.1.1.1   emr-as.pre.cn-qingdao.aliyuncs.com
1.1.1.1   emr-as.pre.cn-beijing.aliyuncs.com
1.1.1.1   emr-as.pre.cn-shanghai.aliyuncs.com
1.1.1.1   emr-as.pre.cn-hongkong.aliyuncs.com
1.1.1.1   emr-as.pre.us-west-1.aliyuncs.com

1.1.1.1   emr.ap-southeast-1.aliyuncs.com
1.1.1.1   emr.cn-shenzhen.aliyuncs.com
1.1.1.1   emr.cn-qingdao.aliyuncs.com
1.1.1.1   emr.cn-beijing.aliyuncs.com
1.1.1.1   emr.cn-shanghai.aliyuncs.com
1.1.1.1   emr.cn-hongkong.aliyuncs.com
1.1.1.1   emr.us-west-1.aliyuncs.com

1.1.1.1   emr-as.ap-southeast-1.aliyuncs.com
1.1.1.1   emr-as.cn-shenzhen.aliyuncs.com
1.1.1.1   emr-as.cn-qingdao.aliyuncs.com
1.1.1.1   emr-as.cn-beijing.aliyuncs.com
1.1.1.1   emr-as.cn-shanghai.aliyuncs.com
1.1.1.1   emr-as.cn-hongkong.aliyuncs.com
1.1.1.1   emr-as.us-west-1.aliyuncs.com
#start add cluster host of cluster 36293,Sun Jan 01 21:55:12 CST 2017
10.27.71.28  emr-worker-5.cluster-36293 emr-worker-5 iZbp13nt8kraa4mucn0jywZ
10.27.70.35  emr-worker-21.cluster-36293 emr-worker-21 iZbp11nfqrmmcl5igubekbZ
10.27.71.162  emr-worker-12.cluster-36293 emr-worker-12 iZbp11nfqrmmcl1kem7fwjZ
10.27.71.93  emr-worker-14.cluster-36293 emr-worker-14 iZbp1gp7tdd3qspvt3h7yuZ
10.27.71.54  emr-worker-17.cluster-36293 emr-worker-17 iZbp16lj061gs3j5tdhdn9Z
10.27.71.108  emr-worker-2.cluster-36293 emr-worker-2 emr-header-3 iZbp108l83nak4aljhz5ltZ
10.27.71.161  emr-header-1.cluster-36293 emr-header-1 iZbp16l88xgquw1qzwm6yjZ
10.27.71.95  emr-worker-8.cluster-36293 emr-worker-8 iZbp167av7uyf8kovzrtr6Z
10.27.71.131  emr-worker-11.cluster-36293 emr-worker-11 iZbp10esjffv301wonx1aaZ
10.27.71.98  emr-worker-13.cluster-36293 emr-worker-13 iZbp19uuc43mjbbwaa5xd1Z
10.27.71.86  emr-worker-20.cluster-36293 emr-worker-20 iZbp10esjffv305uqw0znaZ
10.27.71.113  emr-worker-6.cluster-36293 emr-worker-6 iZbp13sw4kezzcq8gtqycmZ
10.27.71.140  emr-worker-22.cluster-36293 emr-worker-22 iZbp13nt8kraa4wxebqnvjZ
10.27.71.227  emr-worker-24.cluster-36293 emr-worker-24 iZbp15bor20enlxiy0acisZ
10.27.71.215  emr-worker-4.cluster-36293 emr-worker-4 iZbp1459u86is1csh19z1vZ
10.27.71.7  emr-worker-10.cluster-36293 emr-worker-10 iZbp10esjffv301wonx1a9Z
10.27.71.175  emr-worker-16.cluster-36293 emr-worker-16 iZbp12mkw2q2ocfh33blu2Z
10.27.71.163  emr-worker-3.cluster-36293 emr-worker-3 iZbp11nfqrmmcjzsybllh5Z
10.27.69.124  emr-worker-23.cluster-36293 emr-worker-23 iZbp14th42unp31qmati8eZ
10.27.71.130  emr-worker-9.cluster-36293 emr-worker-9 iZbp1ia29ih0mr0t2ib7rqZ
10.27.70.89  emr-worker-19.cluster-36293 emr-worker-19 iZbp1jc97shz65huu60wygZ
10.27.71.211  emr-worker-18.cluster-36293 emr-worker-18 iZbp19uuc43mjbe56js6o3Z
10.27.71.106  emr-worker-7.cluster-36293 emr-worker-7 iZbp1459u86is28exv9o4eZ
10.27.71.100  emr-worker-1.cluster-36293 emr-worker-1 emr-header-2 iZbp1j9eycwskqr570uxwjZ
10.27.71.183  emr-worker-15.cluster-36293 emr-worker-15 iZbp11nfqrmmcl3tavtp8dZ
#end add cluster host

[hadoop@emr-worker-20 ~]$ cat /etc/hosts
127.0.0.1 localhost
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.46.79.84 iZ23vt9ykohZ
10.46.71.22 iZ23o7qodjnZ
10.25.6.165 iZ23j3z85poZ
10.25.113.77 iZ25h5j1pblZ
10.25.197.239 iZ2582bjjvsZ
10.25.233.107 iZ23fcl3kmtZ
10.25.84.144 iZ239jrw8q2Z
10.26.248.27 iZ23xqt8umoZ
10.28.108.176 iZbp1ielabd9elwzi6s9vjZ
# todo delete when meta service ready
1.1.1.1   emr.pre.ap-southeast-1.aliyuncs.com
1.1.1.1   emr.pre.cn-shenzhen.aliyuncs.com
1.1.1.1   emr.pre.cn-qingdao.aliyuncs.com
1.1.1.1   emr.pre.cn-beijing.aliyuncs.com
1.1.1.1   emr.pre.cn-shanghai.aliyuncs.com
1.1.1.1   emr.pre.cn-hongkong.aliyuncs.com
1.1.1.1   emr.pre.us-west-1.aliyuncs.com

1.1.1.1   emr-as.pre.ap-southeast-1.aliyuncs.com
1.1.1.1   emr-as.pre.cn-shenzhen.aliyuncs.com
1.1.1.1   emr-as.pre.cn-qingdao.aliyuncs.com
1.1.1.1   emr-as.pre.cn-beijing.aliyuncs.com
1.1.1.1   emr-as.pre.cn-shanghai.aliyuncs.com
1.1.1.1   emr-as.pre.cn-hongkong.aliyuncs.com
1.1.1.1   emr-as.pre.us-west-1.aliyuncs.com

1.1.1.1   emr.ap-southeast-1.aliyuncs.com
1.1.1.1   emr.cn-shenzhen.aliyuncs.com
1.1.1.1   emr.cn-qingdao.aliyuncs.com
1.1.1.1   emr.cn-beijing.aliyuncs.com
1.1.1.1   emr.cn-shanghai.aliyuncs.com
1.1.1.1   emr.cn-hongkong.aliyuncs.com
1.1.1.1   emr.us-west-1.aliyuncs.com

1.1.1.1   emr-as.ap-southeast-1.aliyuncs.com
1.1.1.1   emr-as.cn-shenzhen.aliyuncs.com
1.1.1.1   emr-as.cn-qingdao.aliyuncs.com
1.1.1.1   emr-as.cn-beijing.aliyuncs.com
1.1.1.1   emr-as.cn-shanghai.aliyuncs.com
1.1.1.1   emr-as.cn-hongkong.aliyuncs.com
1.1.1.1   emr-as.us-west-1.aliyuncs.com
#start add cluster host of cluster 36293,Sun Jan 01 21:55:12 CST 2017
10.27.71.28  emr-worker-5.cluster-36293 emr-worker-5 iZbp13nt8kraa4mucn0jywZ
10.27.70.35  emr-worker-21.cluster-36293 emr-worker-21 iZbp11nfqrmmcl5igubekbZ
10.27.71.162  emr-worker-12.cluster-36293 emr-worker-12 iZbp11nfqrmmcl1kem7fwjZ
10.27.71.93  emr-worker-14.cluster-36293 emr-worker-14 iZbp1gp7tdd3qspvt3h7yuZ
10.27.71.54  emr-worker-17.cluster-36293 emr-worker-17 iZbp16lj061gs3j5tdhdn9Z
10.27.71.108  emr-worker-2.cluster-36293 emr-worker-2 emr-header-3 iZbp108l83nak4aljhz5ltZ
10.27.71.161  emr-header-1.cluster-36293 emr-header-1 iZbp16l88xgquw1qzwm6yjZ
10.27.71.95  emr-worker-8.cluster-36293 emr-worker-8 iZbp167av7uyf8kovzrtr6Z
10.27.71.131  emr-worker-11.cluster-36293 emr-worker-11 iZbp10esjffv301wonx1aaZ
10.27.71.98  emr-worker-13.cluster-36293 emr-worker-13 iZbp19uuc43mjbbwaa5xd1Z
10.27.71.86  emr-worker-20.cluster-36293 emr-worker-20 iZbp10esjffv305uqw0znaZ
10.27.71.113  emr-worker-6.cluster-36293 emr-worker-6 iZbp13sw4kezzcq8gtqycmZ
10.27.71.140  emr-worker-22.cluster-36293 emr-worker-22 iZbp13nt8kraa4wxebqnvjZ
10.27.71.227  emr-worker-24.cluster-36293 emr-worker-24 iZbp15bor20enlxiy0acisZ
10.27.71.215  emr-worker-4.cluster-36293 emr-worker-4 iZbp1459u86is1csh19z1vZ
10.27.71.7  emr-worker-10.cluster-36293 emr-worker-10 iZbp10esjffv301wonx1a9Z
10.27.71.175  emr-worker-16.cluster-36293 emr-worker-16 iZbp12mkw2q2ocfh33blu2Z
10.27.71.163  emr-worker-3.cluster-36293 emr-worker-3 iZbp11nfqrmmcjzsybllh5Z
10.27.69.124  emr-worker-23.cluster-36293 emr-worker-23 iZbp14th42unp31qmati8eZ
10.27.71.130  emr-worker-9.cluster-36293 emr-worker-9 iZbp1ia29ih0mr0t2ib7rqZ
10.27.70.89  emr-worker-19.cluster-36293 emr-worker-19 iZbp1jc97shz65huu60wygZ
10.27.71.211  emr-worker-18.cluster-36293 emr-worker-18 iZbp19uuc43mjbe56js6o3Z
10.27.71.106  emr-worker-7.cluster-36293 emr-worker-7 iZbp1459u86is28exv9o4eZ
10.27.71.100  emr-worker-1.cluster-36293 emr-worker-1 emr-header-2 iZbp1j9eycwskqr570uxwjZ
10.27.71.183  emr-worker-15.cluster-36293 emr-worker-15 iZbp11nfqrmmcl3tavtp8dZ
#end add cluster host

[hadoop@emr-worker-20 ~]$ exit
logout
Connection to 10.27.71.86 closed.
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201701012318.txt 
BLOSUM62	/queryD/D9L10240N4	/Luniref/DL9Line.fasta	256	1	5	11	1	
topK:5 Query:UniRef100_K1PNP9                                                   
AlignmentRecord(refName=UniRef100_K1PNP9, refSequence.take(100)=MFSCFRKKKKKLKGKPPLASEAVVSVKEPQRQATQQISDVVVSGQEDLSAKEALLLWSRRTVEGYPGVKVKNFSSSWRDGRAFLSIIHRHRPDLVDFRKV, cigar=10247M, score1=53844, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_V4A6K8, refSequence.take(100)=MLKYLTCGGCGCGRKKKKPKNLNGDNAPYAPAEKPLISSTVVPGQPQDISARDALLYWSRRTTEGYPGIKITDFSKSWRDGKAFLAIIHRNRPDLVDFRK, cigar=6M2D5M1D5M5I5M6I10M1D812M9D30M2D143M2D189M2I82M8I7M3I35M1D9M3D505M1I48M1D12M5I15M1D2M4I8M1D62M1I23M1D232M4D196M3D7M1D15M2I19M21I9M6I3M15I5M39I2M29I3M6I154M1I41M3I5M1D7M3I13M2I3M4I10M6I24M1D12M5I47M7I9M8I13M5I46M1I35M3I27M6I3M5I20M32I2M1D7M4I11M18I108M1I5M12D36M5D18M3D53M4I12M5D78M1I30M4I28M3D14M17D5M2D30M11D9M15D5M14D241M6I49M8D9M10D2M1D7M11D29M5D60M2I67M2D13M1I60M1D151M8D85M1D18M24I7M11I14M1I4M4I12M3I41M1D26M1D24M2D8M2I10M3I37M1I72M1D3M4D7M11D7M15D14M39D6M26D8M13D4M7D5M2D5M35D7M16D3M89D52M9D10M3D26M1I18M5D22M57D31M27D10M1I5M25D3M12D17M12D22M16D21M2I189M3D158M1D177M22I13M25I8M11I28M1I13M6D54M4I9M24I9M5I2M3I7M7I32M27I7M14I9M2I5M22I13M7I20M14I4M9I21M1D89M5D12M4I7M1D12M1D131M1I46M53D5M20D10M19D5M27D9M106D7M20D7M2D9M5D7M2D4M18D16M36D6M21D13M13D22M37D13M5D5M63D8M44D5M4D658M1I30M1D8M5I484M1D10M1I175M1D1200M1D499M2D151M1D8M1D9M11D7M4D4M24D138M2D81M20I35M1D7M3I27M4I14M1I5M4D29M1I17M1D41M, score1=18535, score2=8384, ref_begin1=3, ref_end1=10815, read_begin1=3, read_end1=10217, ref_end2=5684)
AlignmentRecord(refName=UniRef100_A0A0L8I8F9, refSequence.take(100)=MRFHKMQNVQISLDFLRYKGIKLVNIRADEIVDGNPKLTLGLIWTIILHFQISDVVVPGQPENLTAKEALLLWSRRTTEGYPGIKVTDFSTSWRDGKAFL, cigar=10M1D85M14I5M2I5M2D403M1D298M9D20M3D9M1I36M1I104M2I10M5D133M4I105M5I22M1I8M1I24M4I157M1D286M1I30M1I92M5D17M1D10M6D43M2I119M1D2M2I28M1D331M9D10M6I13M1D32M9I12M11I76M5I42M1I163M4D33M26D11M35D15M58D9M7D11M30D7M25D4M14D5M38D9M41D22M4D8M18D3M42D20M32D10M34D4M1D14M6D3M2D6M2D47M2D46M1D3M10I18M15I12M9I26M30I97M11D21M4I34M15D7M4D7M7D11M27D49M2D82M1I19M3D12M1D9M1D8M12D40M2D9M24D21M14D13M1D231M1D10M2I60M9I2M11I6M22I58M2I81M2I50M1D12M2I133M4D91M1D33M2I8M1I6M3D14M1D7M3I7M1I7M17I31M1D20M2D54M2I37M1I70M16D3M33D11M7D6M3I20M36D5M13D52M31D45M5I14M7I31M3D12M13D27M6D12M15D7M7D11M9D9M5D24M1I88M1I38M1I23M1I8M1I30M3D63M1I71M2I22M1I106M1I15M1I71M1D7M1D28M7D6M10D23M17D151M4D3M20D17M2D18M1D10M2I9M1D4M2D10M10D11M5D24M50D9M2D11M35D9M37D12M2D6M32D10M6D15M4D6M1D14M26D8M3D7M6D4M50D4M28D5M6D5M14D3M128D9M5D60M5D14M6D7M6D149M1I53M1D10M8D5M7D2M1D17M4D4M1D14M14D12M3D21M13D13M2D8M11D22M10D13M2I147M2I510M1I8M3I20M2I7M1D3M1D460M1D221M1I11M4D120M3I11M3D284M1I762M1D653M1I9M4I11M1I214M20I40M4D20M2I9M3I17M2I9M3D10M3D13M2I55M6I18M, score1=16995, score2=7811, ref_begin1=35, ref_end1=11382, read_begin1=35, read_end1=10239, ref_end2=6245)
AlignmentRecord(refName=UniRef100_A0A0L8I9U6, refSequence.take(100)=MLLNRNFPSTLIEMRSLLADFSRFRLEEVPPRLELKQKLFRLFDEIQALSNDTLPLGLEDELYPKNVERLFSRFLLAQQEKDMAIQSELVRLERLQRLAE, cigar=365M1D298M9D20M3D9M1I36M1I104M2I10M5D133M4I105M5I22M1I8M1I24M4I157M1D286M1I30M1I92M5D17M1D10M6D43M2I119M1D2M2I28M1D331M9D10M6I13M1D32M9I12M11I76M5I42M1I163M4D17M4I22M6I4M4I6M3I6M6I11M1D9M3I25M1I43M2I5M4D19M4I9M2I12M38I16M19I6M7I12M1D18M4I21M5I33M7I20M4I5M1D2M16I29M6I18M12I10M11I9M12I5M2I4M11I4M6I16M1I10M10I17M13I4M3I16M11I6M20I4M3I2M24I26M2I25M12I12M2I13M2D30M9I53M3D3M7I38M3D23M3D8M1D5M1I59M19I28M1D21M3I7M4I3M13I15M16I43M6D26M2I11M12I8M5I9M2I6M5I17M1I54M4I3M6I2M8I37M22I24M1D7M25I16M1I9M42I5M6I10M43I14M3I12M10I3M10I41M1D10M12I5M12I12M9I4M14I4M57I41M22I22M9I39M3D10M4D12M3I19M2I7M11I6M18I21M8I9M5I5M16I11M14I20M2I37M3D4M16D46M3I3M10I6M2D53M29I10M12I16M10I6M3I22M1D31M33I27M7I12M12I15M1I3M29I15M7I23M46I15M1D13M26I10M6I6M15I11M43I22M1I7M3I4M16I15M25I14M2I21M1D22M15I3M4I10M3I38M12I7M12I16M31I12M18I39M7I28M5D21M17I5M19I53M20I6M72I15M14I15M1I20M1I15M2I4M5I16M2I6M1D16M21I147M2I510M1I8M3I20M2I7M1D3M1D460M1D221M1I11M4D120M3I11M3D284M1I762M1D653M1I9M4I11M1I274M4D20M2I9M3I17M2I9M3D10M3D13M2I55M6I18M, score1=13993, score2=5071, ref_begin1=194, ref_end1=8687, read_begin1=194, read_end1=10239, ref_end2=3554)
AlignmentRecord(refName=UniRef100_A0A0L8I8R7, refSequence.take(100)=MLLNRNFPSTLIEMRSLLADFSRFRLEEVPPRLELKQKLFRLFDEIQALSNDTLPLGLEDELYPKNVERLFSRFLLAQQEKDMAIQSELVRLERLQRLAE, cigar=365M1D298M9D20M3D9M1I36M1I104M2I10M5D133M4I105M5I22M1I8M1I24M4I157M1D286M1I30M1I92M5D17M1D10M6D43M2I119M1D2M2I28M1D331M9D10M6I13M1D32M9I12M11I76M5I42M1I163M4D17M4I22M6I4M4I6M3I6M6I11M1D9M3I25M1I43M2I5M4D19M4I9M2I12M38I16M19I6M7I12M1D18M4I21M5I33M7I20M4I5M1D2M16I29M6I18M12I10M11I9M12I5M2I4M11I4M6I16M1I10M10I17M13I4M3I16M11I6M20I4M3I2M24I26M2I25M12I12M2I13M2D30M9I53M3D3M7I38M3D23M3D8M1D5M1I59M19I28M1D21M3I7M4I3M13I15M16I43M6D26M2I11M12I8M5I9M2I6M5I17M1I54M4I3M6I2M8I37M22I24M1D7M25I16M1I9M42I5M6I10M43I14M3I12M10I3M10I41M1D10M12I5M12I12M9I4M14I4M57I41M22I22M9I39M3D10M4D12M3I19M2I7M11I6M18I21M8I9M5I5M16I11M14I20M2I37M3D4M16D46M3I3M10I6M2D53M29I10M12I16M10I6M3I22M1D31M33I27M7I12M12I15M1I3M29I15M7I23M46I15M1D13M26I10M6I6M15I11M43I22M1I7M3I4M16I15M25I14M2I21M1D22M15I3M4I10M3I38M12I7M12I16M31I12M18I39M7I28M5D21M17I5M19I53M20I6M72I15M14I15M1I20M1I15M2I4M5I16M2I6M1D16M21I147M2I510M1I8M3I20M2I7M1D3M1D460M1D221M1I11M4D120M3I11M3D284M1I762M1D653M1I9M4I11M1I214M20I40M4D20M2I9M3I17M2I9M3D10M3D13M2I55M6I18M, score1=13945, score2=5059, ref_begin1=194, ref_end1=8667, read_begin1=194, read_end1=10239, ref_end2=3543)

topK:5 Query:UniRef100_A0A0L1HXI4
AlignmentRecord(refName=UniRef100_A0A0L1HXI4, refSequence.take(100)=MWSRLTGSSAASNSNKDDDSRRRRTSDSTRSKRDRDPDTRSMVSSTSTRKPSSTPRRDAAPSSSSIASFTTAFDDMPRSRVSANTNPNSDRYDDADAAHD, cigar=10244M, score1=54065, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_E3RQI2, refSequence.take(100)=MWSRLTGSSAASSSSKDEDSRRRRTSDSTRSKRDRETGDTRSVVSSTSTRKPPSTRRETAGSSIVSFATAFDEMPRSRATTNTNPNTSTNSDLYDDPENY, cigar=37M1D17M1I5M2I25M4D9M5I22M1D44M1D12M1D25M10I56M1I83M1D126M2D21M1D9M1I101M1D8M3I90M3I73M1D24M1I62M6I153M2D113M1I34M3D58M4D142M3D52M2D84M1D24M16D42M2I136M3I120M9D17M1I9M14I37M1D3M3I40M1I20M2I17M1D12M4D33M2I38M111D90M5I47M1I23M3D28M5D46M11D3M1I9M1I46M1I55M1I32M1D24M16I157M1I19M1I50M1D34M16I71M1I56M2I26M1I56M3D14M1I19M2D2M11D13M11D10M3D30M3I5M1I12M4D10M4D4M5I39M4I14M2I11M1I23M6I31M23D32M9D5M12D4M11D3M16D14M3D7M4D11M4D9M8D97M1I31M22I11M10I18M1I5M6I18M3I21M1D9M1I13M84I7M85I4M37I96M8D48M11I61M1D11M1D12M1D12M23D8M1I16M1I43M14D11M20D13M1D21M4D15M1I28M6D10M2D33M44D14M1I32M1D15M1D26M10D7M1I22M2I25M5I10M2D13M1I72M3D5M3D60M6D8M8D20M6D17M2D11M3D6M1I61M1D2M20D29M16D7M49D4M51D73M2I43M1I26M3I22M3D27M4D16M3D4M6D4M1I62M1I25M2D7M1D33M3D18M2D6M3D38M1D8M5D50M1D35M1I19M7I11M11I16M3I11M13I24M4I27M2I10M2I18M15I5M28I13M2I51M1I42M2I17M4D50M14D84M6D37M6I5M35I7M14I16M1D94M3D30M4D8M4D10M7D49M1D5M2I7M1I3M11D44M3D2M5D7M2D6M5D13M6I24M2I3M6D22M1D5M1I13M1D10M1D27M1D53M1I8M2I45M5I32M7I2M3I11M5I49M2I6M6I15M8I48M30D39M1D17M1D6M1D35M3I51M12I2M4I15M2I14M3D70M2D24M3D44M1D15M1D12M2D4M1D60M1D65M1D35M4I24M1D52M3D32M2I18M1I17M4I17M2I15M2I20M1I15M4D3M2D18M1D13M2I5M8D10M1I7M9D10M7D106M1D18M1D10M8I10M1D6M6I77M4D34M4I5M2D26M1D4M23I4M12I6M1D12M2I5M5I40M7I3M5I17M27I7M35I11M31I9M7I22M1I25M2I5M21I16M12I8M7I9M1D55M1I41M18I13M5I21M1D16M1D20M1D7M3D87M1D3M8I32M9D226M3I5M4D39M6D39M3D22M1I11M1I7M3I53M11D10M7D150M2D6M3I5M1D64M2D8M5I69M2I43M2D5M2D584M9D21M, score1=22332, score2=12922, ref_begin1=0, ref_end1=9818, read_begin1=0, read_end1=9834, ref_end2=4686)
AlignmentRecord(refName=UniRef100_B2VU89, refSequence.take(100)=MWSRLTGSSAASNSSKDEESRRRRTSDSTRSKRDRETGEARSVVSSTSTRKPPSTRRETAGSSIVSFATAFDEMPRSRATTNTNPNTNTNSDLYDDPENY, cigar=37M1D17M1I5M2I25M4D9M5I22M1D44M1D37M10I56M1I140M15I186M1D7M2I92M3I45M1I27M1D24M1I62M6I38M1I114M2D113M1I30M3D62M4D142M3D52M2D84M1D38M15D28M2I136M3I120M9D17M1I9M14I37M5I24M16D17M1I20M2I17M1D12M4D33M2I36M111D92M6I5M1D41M1I23M3D28M5D40M5D10M5D9M1I46M1I55M1I32M1D22M18I163M1I3M4D10M1I50M1D33M1D15M1D73M1I52M2I30M1I56M3D14M1I19M13D23M1I31M3I5M1I12M4D10M1I47M4I23M3D6M6I19M9I25M23D33M9D4M12D4M35D4M4D7M4D10M2D3M13D111M1I39M29I5M3I22M1I17M6I6M3I21M1D6M1I13M48I2M36I8M85I4M37I57M1I38M8D48M11I59M1D16M1D10M1D11M17D25M1I45M2D9M32D13M1D21M1I22M3D21M6D10M2D34M44D13M1I32M2I11M7I9M3I13M1I7M5D10M2I10M2D12M7D14M5D15M13D22M1I20M2I7M1I57M15D16M3D33M8I9M3D7M3D7M2I9M1D5M2D34M3I6M3D21M60D19M20D4M44D9M6D12M4D73M2I45M6D5M3D23M2I41M5I25M1I37M5I46M2D40M3D7M2D14M4D41M1D19M5D39M1D46M83I84M4I45M2D12M3I29M10I12M9D43M2I17M4D17M9D13M8D4M3I45M1I10M1D47M6D35M56I25M1D45M1D22M6D55M2D6M4D12M5D49M1D3M4D3M6D11M1D4M4D7M8D19M2D7M2I12M3D26M8I28M5D17M1D5M1I13M1D24M1D21M5I23M1I25M2I47M5I30M10I13M5I62M10I3M4I49M14I47M1D5M1D33M3I47M8I3M1I27M4I14M3D57M2D9M1D25M2D33M1I13M1D17M1D8M2D5M1D20M1I105M1D36M3I5M1I15M1D65M6I18M12I17M3D9M4I15M2I11M2I31M2D9M2I10M1D4M4D6M1D10M13D7M7D5M5D12M2I10M7D107M1D18M1D10M5I6M8I88M4D34M4I5M2D23M1D7M23I6M11I31M8I32M8I4M45I6M24I6M34I15M1D8M7I18M1I30M20I18M2I10M20I8M1D39M3D5M2D11M6I6M15I14M3I22M3I22M1D13M2I10M1D21M6D8M2I46M1I36M1D3M8I258M3I3M4D41M5D4M1D31M2D6M1D18M2I19M3I54M11D8M7D158M3I5M1D65M3I78M2I43M2D5M2D545M, score1=21877, score2=12720, ref_begin1=0, ref_end1=9656, read_begin1=0, read_end1=9774, ref_end2=4527)
AlignmentRecord(refName=UniRef100_W6YRA0, refSequence.take(100)=MWSRLTGSSGASQSPKDDDARRRRTTTSDSTRSKRDRDRDPDTRSVVSSTSTRKPSVSRRDTAPSSTASFTTAFDDIPRSRAAPNTDPNSDYDRRRDDRR, cigar=24M2D8M2D19M1I12M2I24M12I25M1D14M1I71M10D3M4I262M4D136M3I96M4I14M2I76M1I223M1D96M6I10M4D30M2I95M6I35M67D117M2D90M3I15M1D43M2I193M5I41M2D36M16I42M6I83M3I20M6D28M1D41M6I221M1I11M8D11M1I34M4D59M4I2M2I28M1D27M20I53M1D26M1I26M3I118M1D33M1D85M1I28M10D94M1I61M5I9M1I21M5I114M5I16M1D2M6I20M1D5M2D9M1D36M8D13M26D20M1D45M2D26M4D32M4D4M1I6M5I13M53I7M16I4M46I12M1D14M30I2M14I21M1D11M35I18M7I5M69I31M2I13M2D9M4D20M8I19M8I11M10I13M3I26M1D24M1I16M1D23M2I16M4I3M7I6M5I11M40I16M1D22M2D8M5D4M1I32M6D24M4I6M4I37M62I8M5I9M82I5M5I80M2D15M17I8M1D51M1D8M13D6M5D9M1I19M23D41M10D5M73D53M1D26M55I8M10I15M18I16M7I12M4I7M7I34M1D68M1D31M9D38M24I6M6I28M19I15M24I6M10I7M17I2M8I11M46I10M27I30M9I6M1I3M20I13M1D10M29I9M56I14M34I22M2D17M2I28M1D21M1D7M3I4M9I8M2I8M1I11M25I6M6I18M5I113M1I69M1D23M3D21M3I8M7I3M1I12M1D35M1I32M5D13M3I7M3I9M1I11M4D41M8I14M10I12M1I23M25I16M15I37M1I20M28I2M5I42M1D26M31I39M22I3M25I107M2I10M14I11M2I8M3I17M5I49M1D12M1D72M1D121M1I8M3I52M3D82M4I81M1D11M16I8M2I19M4D41M1I45M20I21M5I8M17I60M6I34M3I32M1D20M11I25M8I5M33I17M12I2M2I27M1I23M26I2M47I8M28I13M1I16M9I8M16I10M7I5M3I9M2I15M6I17M25I27M4I3M68I5M20I5M3I4M17I6M6I78M21D194M3I65M3I2M28I91M9I53M2I19M49I4M6I15M1D8M1I55M2I70M1D14M4D34M1I101M1D586M, score1=21443, score2=11496, ref_begin1=0, ref_end1=8309, read_begin1=0, read_end1=9832, ref_end2=3184)
AlignmentRecord(refName=UniRef100_E5ACN6, refSequence.take(100)=MWSRFTGKSDSSSSASAKDRDEDARRRRRPSESTRSKRERDSDARSVVSSTSTRKPSRRDTAPSSIASFATAFDDVPRSQPTHEVYDSGRDDRYPSSPST, cigar=7M4D13M1D32M3I6M2I17M6I6M11I7M3I16M5D72M18I94M4D64M1D7M1I6M3I15M1I7M2I17M3I59M1I20M2D145M14D7M1I39M6I44M1I26M1D25M1I73M8D31M2D232M3D82M33I18M13I20M5I6M6I23M5D39M1I38M2D13M1D75M5I11M3D5M7D2M9D13M34D35M4I245M2I23M6I3M7I22M1D16M35D4M1D12M16D9M8D15M4I24M1I15M2I27M3I20M1D17M2D21M6I15M14I94M1D13M1I43M1I31M10D5M18D7M13D55M8D73M2I14M2I12M2D23M7I3M11I21M1I51M2I87M1D12M1D50M1D23M1D9M2I88M1I123M2D30M3I32M1D23M2I49M1D6M1I27M7I30M5D15M7D19M9I14M2D9M1I7M14D20M5D4M1D6M4D31M9D45M6I25M4I14M16I6M14I6M2I15M3I17M9I2M1I49M38I4M12I24M1I6M10I10M15I17M1I47M5I11M1D18M8I4M20I14M7I24M11I11M1I10M2I11M22I34M2D29M10D44M2D12M24I15M1I14M3D10M17I3M4I6M1I7M4I4M5I17M2D6M2I5M17I15M1I15M1I4M1D10M5I15M4D13M3I6M3I17M3I12M2I23M6D13M3I37M1I6M4D17M5D38M5D22M3D9M2D57M3I4M1D7M5D9M9D11M3I4M7D7M1D45M5D21M6D4M4D9M3D50M2D21M1I8M18D41M1I17M5D8M6D5M1D16M1I11M9D45M7D26M1D4M1D41M1I10M2I23M21I33M3D12M2I26M1I4M4D21M5D20M1D4M3D11M2I17M2I8M1I3M8I20M1I22M9D60M8D10M5D5M3D5M13D10M4D36M12D10M3D15M3D29M5D16M5D11M2D30M5I37M1I5M10D14M1I22M11D45M3D3M5D3M1D16M4D3M5D10M3D40M2I9M1D45M15D3M13D11M5D6M1D3M3D17M5D3M16D4M1D37M19D29M1D6M6D8M2D11M5I11M1D23M10D3M4D17M1I7M2D26M2D31M1I25M2D14M2I12M4D25M5D12M16I33M3I16M1I11M3I14M3I53M2I18M2D5M1I15M3I36M2I20M3I12M8I12M3D7M7I10M2D4M2D7M1I7M2D29M5D6M3I17M1I24M3D15M12D3M17D28M2D10M3D17M1D7M3D3M1I22M10I6M25I9M1D17M5I2M2I33M1D10M4I23M4I62M5I14M2I9M3I5M3I12M9I6M3I2M6I8M3I49M6D29M11D35M3I26M14D16M1D21M3I13M3D38M1I8M5I11M1D39M4D18M5D25M1I11M1I39M23D29M2D5M1I10M7D63M4D50M1I20M2D3M1I14M4D20M1I6M1I10M1D15M1D7M5D19M3I42M4D65M12I22M1D16M3I10M3I9M2I7M16I8M11I8M2I34M18I17M2D66M9I28M5I13M4I2M20I7M2D8M2I5M5I4M3D24M1I55M8D24M8D10M7D19M5D17M5D17M1I16M1D10M3D11M5D8M1D27M5I5M3D63M1D3M8I14M8I155M1D5M2D13M9D7M22D55M2D17M3I28M2D4M6I3M2I27M37I9M12I21M8I7M12I47M1D34M5I19M2I33M2I93M2D115M2D28M1D4M19D119M36I373M1I34M1I7M, score1=16323, score2=9878, ref_begin1=0, ref_end1=9739, read_begin1=0, read_end1=9825, ref_end2=4604)

topK:5 Query:UniRef100_A0A0L7M5X2
AlignmentRecord(refName=UniRef100_A0A0L7M5X2, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=10240M, score1=55757, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_W4IKS5, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=8535M43D1705M, score1=55646, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_W7G1W4, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=588M1D191M7D897M1D1400M2I3396M2I199M2D512M1D376M7D972M43D749M2I954M, score1=55409, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_W7JS47, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=588M1D191M15D897M1D1400M2I2796M15D8M1D592M2I711M1D387M2D961M43D749M2I636M1D318M, score1=55378, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_A0A0L7K719, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=587M1I191M21D2297M2I1038M2D2358M2I1097M1I961M43D749M2I636M2D317M, score1=55385, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)

topK:5 Query:UniRef100_A0A0L0CSA9
AlignmentRecord(refName=UniRef100_A0A0L0CSA9, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=10229M, score1=55680, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_W4IKS5, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=3076M2D106M20D3179M19D72M2D864M1D1M7D222M3D1753M2D635M1D315M, score1=55320, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_A0A0L7K719, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=587M1I191M21D2403M20D912M2D2267M19D936M1D1M7D222M2D2388M3D315M, score1=55283, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_W7G1W4, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=588M1D191M7D897M1D1506M20D3179M19D271M2D512M1D153M1D1M7D213M8D1M2D2396M1D315M, score1=55289, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)
AlignmentRecord(refName=UniRef100_W7JS47, refSequence.take(100)=MKDLYSLYEDISENDKIIISEKLKNLLNYFQDKNKSYVSEFVVVSRTRFFKSISNYGEFLLLQSSSRVISSYEHILRLLHQAKVYLEFVRCLKLNNCSVI, cigar=588M1D191M15D897M1D1506M20D2670M15D8M1D501M19D783M1D153M1D1M7D222M5D2388M2D315M, score1=55261, score2=0, ref_begin1=0, ref_end1=0, read_begin1=0, read_end1=0, ref_end2=0)

BLOSUM62	/queryD/D9L10240N4	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 1:>                                                       (0 + 88) / 256]Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 1.0 failed 4 times, most recent failure: Lost task 35.3 in stage 1.0 (TID 99, emr-worker-20.cluster-36293): java.lang.NoClassDefFoundError: Could not initialize class parasail.Matrix
	at parasail.RunParasail.createProfile(RunParasail.java:79)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:40)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:37)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30)
	at org.spark-project.guava.collect.Ordering.leastOf(Ordering.java:658)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1391)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1388)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$.main(aliyunDSW2ATMQueryTime.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NoClassDefFoundError: Could not initialize class parasail.Matrix
	at parasail.RunParasail.createProfile(RunParasail.java:79)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:40)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:37)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30)
	at org.spark-project.guava.collect.Ordering.leastOf(Ordering.java:658)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1391)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1388)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
BLOSUM62	/queryD/D9L10240N4	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 1:>                                                       (0 + 88) / 256]Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 1.0 failed 4 times, most recent failure: Lost task 3.3 in stage 1.0 (TID 98, emr-worker-20.cluster-36293): java.lang.NoClassDefFoundError: Could not initialize class parasail.Matrix
	at parasail.RunParasail.createProfile(RunParasail.java:79)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:40)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:37)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30)
	at org.spark-project.guava.collect.Ordering.leastOf(Ordering.java:658)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1391)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1388)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$.main(aliyunDSW2ATMQueryTime.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NoClassDefFoundError: Could not initialize class parasail.Matrix
	at parasail.RunParasail.createProfile(RunParasail.java:79)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:40)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:37)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30)
	at org.spark-project.guava.collect.Ordering.leastOf(Ordering.java:658)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1391)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1388)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 1:======================================================>(255 + 1) / 256]Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 163 in stage 1.0 failed 4 times, most recent failure: Lost task 163.3 in stage 1.0 (TID 289, emr-worker-13.cluster-36293): org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:605)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:595)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:595)
	at org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:580)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:640)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to emr-worker-24.cluster-36293/10.27.71.227:57492
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: emr-worker-24.cluster-36293/10.27.71.227:57492
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryD9L392N40Time.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryD9L392N40Time.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$.main(aliyunDSW2ATMQueryD9L392N40Time.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time.main(aliyunDSW2ATMQueryD9L392N40Time.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.spark.storage.BlockFetchException: Failed to fetch block from 1 locations. Most recent failure cause:
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:605)
	at org.apache.spark.storage.BlockManager$$anonfun$doGetRemote$2.apply(BlockManager.scala:595)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.BlockManager.doGetRemote(BlockManager.scala:595)
	at org.apache.spark.storage.BlockManager.getRemote(BlockManager.scala:580)
	at org.apache.spark.storage.BlockManager.get(BlockManager.scala:640)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:44)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:268)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.io.IOException: Failed to connect to emr-worker-24.cluster-36293/10.27.71.227:57492
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:216)
	at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:167)
	at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:90)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
	at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	... 3 more
Caused by: java.net.ConnectException: Connection refused: emr-worker-24.cluster-36293/10.27.71.227:57492
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	... 1 more
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 1:=====================================================> (248 + 8) / 256]Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 1.0 failed 4 times, most recent failure: Lost task 11.3 in stage 1.0 (TID 301, emr-worker-23.cluster-36293): java.lang.UnsatisfiedLinkError: no JNIparasail in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
	at java.lang.Runtime.loadLibrary0(Runtime.java:870)
	at java.lang.System.loadLibrary(System.java:1122)
	at parasail.JNIparasail.<clinit>(JNIparasail.java:7)
	at parasail.Matrix.lookup(Matrix.java:106)
	at parasail.Matrix.<clinit>(Matrix.java:8)
	at parasail.RunParasail.createProfile(RunParasail.java:79)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:40)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:37)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30)
	at org.spark-project.guava.collect.Ordering.leastOf(Ordering.java:658)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1391)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1388)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryD9L392N40Time.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryD9L392N40Time.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$.main(aliyunDSW2ATMQueryD9L392N40Time.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time.main(aliyunDSW2ATMQueryD9L392N40Time.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.UnsatisfiedLinkError: no JNIparasail in java.library.path
	at java.lang.ClassLoader.loadLibrary(ClassLoader.java:1867)
	at java.lang.Runtime.loadLibrary0(Runtime.java:870)
	at java.lang.System.loadLibrary(System.java:1122)
	at parasail.JNIparasail.<clinit>(JNIparasail.java:7)
	at parasail.Matrix.lookup(Matrix.java:106)
	at parasail.Matrix.<clinit>(Matrix.java:8)
	at parasail.RunParasail.createProfile(RunParasail.java:79)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:40)
	at org.dsa.core.DSW2ATM$$anonfun$1.apply(DSW2ATM.scala:37)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:30)
	at org.spark-project.guava.collect.Ordering.leastOf(Ordering.java:658)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1391)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$29.apply(RDD.scala:1388)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 1:=====================================================> (251 + 5) / 256]Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 107 in stage 1.0 failed 4 times, most recent failure: Lost task 107.3 in stage 1.0 (TID 300, emr-worker-21.cluster-36293): ExecutorLostFailure (executor 33 exited caused by one of the running tasks) Reason: Container marked as failed: container_1483083987918_1232_01_000901 on host: emr-worker-21.cluster-36293. Exit status: 50. Diagnostics: Exception from container-launch.
Container id: container_1483083987918_1232_01_000901
Exit code: 50
Stack trace: ExitCodeException exitCode=50: 
	at org.apache.hadoop.util.Shell.runCommand(Shell.java:545)
	at org.apache.hadoop.util.Shell.run(Shell.java:456)
	at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:722)
	at org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor.launchContainer(DefaultContainerExecutor.java:212)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:302)
	at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:82)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)


Container exited with a non-zero exit code 50

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryD9L392N40Time.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryD9L392N40Time.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time$.main(aliyunDSW2ATMQueryD9L392N40Time.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryD9L392N40Time.main(aliyunDSW2ATMQueryD9L392N40Time.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Exception in thread "yarn-scheduler-ask-am-thread-pool-1" java.lang.Error: java.lang.InterruptedException: sleep interrupted
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1148)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.InterruptedException: sleep interrupted
	at java.lang.Thread.sleep(Native Method)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:114)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:77)
	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.applyOrElse(YarnSchedulerBackend.scala:156)
	at org.apache.spark.scheduler.cluster.YarnSchedulerBackend$YarnSchedulerEndpoint$$anonfun$org$apache$spark$scheduler$cluster$YarnSchedulerBackend$$handleExecutorDisconnectedFromDriver$2.applyOrElse(YarnSchedulerBackend.scala:151)
	at scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:136)
	at scala.concurrent.Future$$anonfun$onFailure$1.apply(Future.scala:134)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	... 2 more
^C
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201701012354Node25.txt 
BLOSUM62	/queryD/D9L10240N4	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 2:>                                                       (0 + 96) / 256]

