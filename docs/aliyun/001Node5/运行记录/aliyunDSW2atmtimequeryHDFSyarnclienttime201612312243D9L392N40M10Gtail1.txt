16/12/31 15:27:36 INFO spark.SecurityManager: Changing view acls to: hadoop
16/12/31 15:27:36 INFO spark.SecurityManager: Changing modify acls to: hadoop
16/12/31 15:27:36 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/12/31 15:27:37 INFO util.Utils: Successfully started service 'sparkDriver' on port 52273.
16/12/31 15:27:37 INFO slf4j.Slf4jLogger: Slf4jLogger started
16/12/31 15:27:37 INFO Remoting: Starting remoting
16/12/31 15:27:37 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@10.27.71.161:40898]
16/12/31 15:27:37 INFO util.Utils: Successfully started service 'sparkDriverActorSystem' on port 40898.
16/12/31 15:27:37 INFO spark.SparkEnv: Registering MapOutputTracker
16/12/31 15:27:37 INFO spark.SparkEnv: Registering BlockManagerMaster
16/12/31 15:27:37 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-97b90898-31d9-434c-a914-2fcb79f047f6
16/12/31 15:27:37 INFO storage.MemoryStore: MemoryStore started with capacity 511.1 MB
16/12/31 15:27:37 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/12/31 15:27:37 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/12/31 15:27:37 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
16/12/31 15:27:37 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
16/12/31 15:27:37 INFO ui.SparkUI: Started SparkUI at http://10.27.71.161:4040
16/12/31 15:27:37 INFO executor.Executor: Starting executor ID driver on host localhost
16/12/31 15:27:37 INFO executor.Executor: Using REPL class URI: http://10.27.71.161:38737
16/12/31 15:27:37 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46010.
16/12/31 15:27:37 INFO netty.NettyBlockTransferService: Server created on 46010
16/12/31 15:27:37 INFO storage.BlockManagerMaster: Trying to register BlockManager
16/12/31 15:27:37 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:46010 with 511.1 MB RAM, BlockManagerId(driver, localhost, 46010)
16/12/31 15:27:37 INFO storage.BlockManagerMaster: Registered BlockManager
16/12/31 15:27:39 INFO scheduler.EventLoggingListener: Logging events to hdfs://emr-header-1:9000/spark-history/local-1483169257926
16/12/31 15:27:39 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
16/12/31 15:27:39 INFO hive.HiveContext: Initializing execution hive, version 1.2.1
16/12/31 15:27:39 INFO client.ClientWrapper: Inspected Hadoop version: 2.7.2
16/12/31 15:27:39 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.2
16/12/31 15:27:40 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:27:40 INFO metastore.HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
16/12/31 15:27:40 INFO metastore.ObjectStore: ObjectStore, initialize called
16/12/31 15:27:40 WARN DataNucleus.General: Plugin (Bundle) "org.datanucleus" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "file:/opt/apps/spark-1.6.2-bin-hadoop2.7/lib/datanucleus-core-3.2.10.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/usr/lib/spark-current/lib/datanucleus-core-3.2.10.jar."
16/12/31 15:27:40 WARN DataNucleus.General: Plugin (Bundle) "org.datanucleus.store.rdbms" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "file:/usr/lib/spark-current/lib/datanucleus-rdbms-3.2.9.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/opt/apps/spark-1.6.2-bin-hadoop2.7/lib/datanucleus-rdbms-3.2.9.jar."
16/12/31 15:27:40 WARN DataNucleus.General: Plugin (Bundle) "org.datanucleus.api.jdo" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "file:/opt/apps/spark-1.6.2-bin-hadoop2.7/lib/datanucleus-api-jdo-3.2.6.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/usr/lib/spark-current/lib/datanucleus-api-jdo-3.2.6.jar."
16/12/31 15:27:40 INFO DataNucleus.Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
16/12/31 15:27:40 INFO DataNucleus.Persistence: Property datanucleus.cache.level2 unknown - will be ignored
16/12/31 15:27:40 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/12/31 15:27:40 WARN DataNucleus.Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
16/12/31 15:27:42 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:27:42 INFO metastore.ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
16/12/31 15:27:43 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/12/31 15:27:43 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/12/31 15:27:44 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
16/12/31 15:27:44 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
16/12/31 15:27:45 INFO metastore.MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
16/12/31 15:27:45 INFO metastore.ObjectStore: Initialized ObjectStore
16/12/31 15:27:45 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
16/12/31 15:27:45 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
16/12/31 15:27:45 INFO metastore.HiveMetaStore: Added admin role in metastore
16/12/31 15:27:45 INFO metastore.HiveMetaStore: Added public role in metastore
16/12/31 15:27:45 INFO metastore.HiveMetaStore: No user is added in admin role, since config is empty
16/12/31 15:27:45 INFO metastore.HiveMetaStore: 0: get_all_databases
16/12/31 15:27:45 INFO HiveMetaStore.audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_all_databases	
16/12/31 15:27:45 INFO metastore.HiveMetaStore: 0: get_functions: db=default pat=*
16/12/31 15:27:45 INFO HiveMetaStore.audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
16/12/31 15:27:45 INFO DataNucleus.Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
16/12/31 15:27:46 INFO session.SessionState: Created local directory: /tmp/e4771921-9ecf-4d8c-a79b-60fc4989ae9a_resources
16/12/31 15:27:46 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/e4771921-9ecf-4d8c-a79b-60fc4989ae9a
16/12/31 15:27:46 INFO session.SessionState: Created local directory: /tmp/hadoop/e4771921-9ecf-4d8c-a79b-60fc4989ae9a
16/12/31 15:27:46 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/e4771921-9ecf-4d8c-a79b-60fc4989ae9a/_tmp_space.db
16/12/31 15:27:46 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:27:46 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
16/12/31 15:27:46 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16/12/31 15:27:46 INFO client.ClientWrapper: Inspected Hadoop version: 2.7.2
16/12/31 15:27:46 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.7.2
16/12/31 15:27:47 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:27:47 INFO hive.metastore: Trying to connect to metastore with URI thrift://emr-header-1:9083
16/12/31 15:27:47 INFO hive.metastore: Connected to metastore.
16/12/31 15:27:47 INFO session.SessionState: Created local directory: /tmp/f8cd4104-160e-424b-99ae-7b387b2a3ddf_resources
16/12/31 15:27:47 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/f8cd4104-160e-424b-99ae-7b387b2a3ddf
16/12/31 15:27:47 INFO session.SessionState: Created local directory: /tmp/hadoop/f8cd4104-160e-424b-99ae-7b387b2a3ddf
16/12/31 15:27:47 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/f8cd4104-160e-424b-99ae-7b387b2a3ddf/_tmp_space.db
16/12/31 15:27:47 INFO repl.SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContext.

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
16/12/31 15:27:52 INFO spark.SparkContext: Invoking stop() from shutdown hook
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
16/12/31 15:27:52 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
16/12/31 15:27:52 INFO ui.SparkUI: Stopped Spark web UI at http://10.27.71.161:4040
16/12/31 15:27:52 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/12/31 15:27:52 INFO storage.MemoryStore: MemoryStore cleared
16/12/31 15:27:52 INFO storage.BlockManager: BlockManager stopped
16/12/31 15:27:52 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
16/12/31 15:27:52 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/12/31 15:27:52 INFO spark.SparkContext: Successfully stopped SparkContext
16/12/31 15:27:52 INFO util.ShutdownHookManager: Shutdown hook called
16/12/31 15:27:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-60f91862-d07a-4b5a-9a68-1af8fbc18a23
16/12/31 15:27:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-76a4ef7b-4c9d-4ab0-be70-74abd7c2d2ff
16/12/31 15:27:52 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-9dff8653-98ce-4a72-aa23-06b9e1913ad1
16/12/31 15:27:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/12/31 15:27:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
16/12/31 15:27:52 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
[hadoop@emr-header-1 spark-1.5.2]$ ./bin/spark
spark-class   sparkR        spark-shell   spark-sql     spark-submit  
[hadoop@emr-header-1 spark-1.5.2]$ ./bin/spark-shell 
16/12/31 15:28:04 INFO spark.SecurityManager: Changing view acls to: hadoop
16/12/31 15:28:04 INFO spark.SecurityManager: Changing modify acls to: hadoop
16/12/31 15:28:04 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/12/31 15:28:04 INFO spark.HttpServer: Starting HTTP Server
16/12/31 15:28:04 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/12/31 15:28:04 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:37292
16/12/31 15:28:04 INFO util.Utils: Successfully started service 'HTTP class server' on port 37292.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.5.2
      /_/

Using Scala version 2.10.4 (OpenJDK 64-Bit Server VM, Java 1.8.0_91)
Type in expressions to have them evaluated.
Type :help for more information.
16/12/31 15:28:08 INFO spark.SparkContext: Running Spark version 1.5.2
16/12/31 15:28:08 INFO spark.SecurityManager: Changing view acls to: hadoop
16/12/31 15:28:08 INFO spark.SecurityManager: Changing modify acls to: hadoop
16/12/31 15:28:08 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(hadoop); users with modify permissions: Set(hadoop)
16/12/31 15:28:08 INFO slf4j.Slf4jLogger: Slf4jLogger started
16/12/31 15:28:08 INFO Remoting: Starting remoting
16/12/31 15:28:09 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.27.71.161:51634]
16/12/31 15:28:09 INFO util.Utils: Successfully started service 'sparkDriver' on port 51634.
16/12/31 15:28:09 INFO spark.SparkEnv: Registering MapOutputTracker
16/12/31 15:28:09 INFO spark.SparkEnv: Registering BlockManagerMaster
16/12/31 15:28:09 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-3f34a19b-6d97-493d-9186-b45dafbb38d1
16/12/31 15:28:09 INFO storage.MemoryStore: MemoryStore started with capacity 530.0 MB
16/12/31 15:28:09 INFO spark.HttpFileServer: HTTP File server directory is /tmp/spark-3bc6cc19-c897-45d0-a07b-77c14853df5f/httpd-6936b9be-69ef-4112-b095-0ca050c8e7e3
16/12/31 15:28:09 INFO spark.HttpServer: Starting HTTP Server
16/12/31 15:28:09 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/12/31 15:28:09 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:39105
16/12/31 15:28:09 INFO util.Utils: Successfully started service 'HTTP file server' on port 39105.
16/12/31 15:28:09 INFO spark.SparkEnv: Registering OutputCommitCoordinator
16/12/31 15:28:09 INFO server.Server: jetty-8.y.z-SNAPSHOT
16/12/31 15:28:09 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
16/12/31 15:28:09 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
16/12/31 15:28:09 INFO ui.SparkUI: Started SparkUI at http://10.27.71.161:4040
16/12/31 15:28:09 WARN metrics.MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
16/12/31 15:28:09 INFO executor.Executor: Starting executor ID driver on host localhost
16/12/31 15:28:09 INFO executor.Executor: Using REPL class URI: http://10.27.71.161:37292
16/12/31 15:28:09 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45411.
16/12/31 15:28:09 INFO netty.NettyBlockTransferService: Server created on 45411
16/12/31 15:28:09 INFO storage.BlockManagerMaster: Trying to register BlockManager
16/12/31 15:28:09 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:45411 with 530.0 MB RAM, BlockManagerId(driver, localhost, 45411)
16/12/31 15:28:09 INFO storage.BlockManagerMaster: Registered BlockManager
16/12/31 15:28:10 INFO scheduler.EventLoggingListener: Logging events to hdfs://emr-header-1:9000/spark-history/local-1483169289438
16/12/31 15:28:10 INFO repl.SparkILoop: Created spark context..
Spark context available as sc.
16/12/31 15:28:11 INFO hive.HiveContext: Initializing execution hive, version 1.2.1
16/12/31 15:28:11 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
16/12/31 15:28:11 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
16/12/31 15:28:11 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:28:11 INFO hive.metastore: Trying to connect to metastore with URI thrift://emr-header-1:9083
16/12/31 15:28:11 INFO hive.metastore: Connected to metastore.
16/12/31 15:28:12 INFO session.SessionState: Created local directory: /tmp/ae54fdf8-e5c9-47ab-89d7-c3c7fa6d4266_resources
16/12/31 15:28:12 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/ae54fdf8-e5c9-47ab-89d7-c3c7fa6d4266
16/12/31 15:28:12 INFO session.SessionState: Created local directory: /tmp/hadoop/ae54fdf8-e5c9-47ab-89d7-c3c7fa6d4266
16/12/31 15:28:12 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/ae54fdf8-e5c9-47ab-89d7-c3c7fa6d4266/_tmp_space.db
16/12/31 15:28:12 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:28:12 INFO hive.HiveContext: default warehouse location is /user/hive/warehouse
16/12/31 15:28:12 INFO hive.HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
16/12/31 15:28:12 INFO client.ClientWrapper: Inspected Hadoop version: 2.6.0
16/12/31 15:28:12 INFO client.ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
16/12/31 15:28:12 WARN conf.HiveConf: HiveConf of name hive.server2.enable.impersonation does not exist
16/12/31 15:28:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/12/31 15:28:12 INFO hive.metastore: Trying to connect to metastore with URI thrift://emr-header-1:9083
16/12/31 15:28:12 INFO hive.metastore: Connected to metastore.
16/12/31 15:28:13 INFO session.SessionState: Created local directory: /tmp/cb358735-72c9-43e6-aacc-696e0e5c64a5_resources
16/12/31 15:28:13 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/cb358735-72c9-43e6-aacc-696e0e5c64a5
16/12/31 15:28:13 INFO session.SessionState: Created local directory: /tmp/hadoop/cb358735-72c9-43e6-aacc-696e0e5c64a5
16/12/31 15:28:13 INFO session.SessionState: Created HDFS directory: /tmp/hive/hadoop/cb358735-72c9-43e6-aacc-696e0e5c64a5/_tmp_space.db
16/12/31 15:28:13 INFO repl.SparkILoop: Created sql context (with Hive support)..
SQL context available as sqlContext.

scala> exit
warning: there were 1 deprecation warning(s); re-run with -deprecation for details
16/12/31 15:28:19 INFO spark.SparkContext: Invoking stop() from shutdown hook
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static/sql,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/execution,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/SQL,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/static,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,null}
16/12/31 15:28:19 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,null}
16/12/31 15:28:19 INFO ui.SparkUI: Stopped Spark web UI at http://10.27.71.161:4040
16/12/31 15:28:19 INFO scheduler.DAGScheduler: Stopping DAGScheduler
16/12/31 15:28:19 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
16/12/31 15:28:19 INFO storage.MemoryStore: MemoryStore cleared
16/12/31 15:28:19 INFO storage.BlockManager: BlockManager stopped
16/12/31 15:28:19 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
16/12/31 15:28:19 INFO spark.SparkContext: Successfully stopped SparkContext
16/12/31 15:28:19 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
16/12/31 15:28:19 INFO util.ShutdownHookManager: Shutdown hook called
16/12/31 15:28:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-7ac3d47f-5b95-4306-ba1b-2eba6deebd40
16/12/31 15:28:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-3bc6cc19-c897-45d0-a07b-77c14853df5f
16/12/31 15:28:19 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1dd7b0d7-dc02-4b13-a1bf-75889202758d
16/12/31 15:28:19 INFO remote.RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
16/12/31 15:28:19 INFO remote.RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ jps
3618 WebAppProxyServer
4242 RunJar
4213 RunJar
3062 ResourceManager
14503 Master
3545 JobHistoryServer
14713 Worker
4793 Bootstrap
4650 HistoryServer
3693 ApplicationHistoryServer
15647 Jps
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/
docker.properties.template    log4j.properties              metrics.properties.backup     slaves                        spark-defaults.conf           spark-defaults.conf.template  .spark-env.sh.swp             
fairscheduler.xml.template    log4j.properties.template     metrics.properties.template   slaves.template               spark-defaults.conf.20160619  spark-env.sh                  spark-env.sh.template         
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/slaves
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-defaults.conf
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-env.sh
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/
docker.properties.template    log4j.properties              metrics.properties.backup     slaves                        spark-defaults.conf           spark-defaults.conf.template  .spark-env.sh.swp             
fairscheduler.xml.template    log4j.properties.template     metrics.properties.template   slaves.template               spark-defaults.conf.20160619  spark-env.sh                  spark-env.sh.template         
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-defaults.conf
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-env.sh
[hadoop@emr-header-1 spark-1.5.2]$ cd conf/
[hadoop@emr-header-1 conf]$ dispatch.sh spark-env.sh
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
[hadoop@emr-header-1 conf]$ cd ../
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ ls logs/
spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Master.out                spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out.4  spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out.3
spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Master.out.1              spark-hadoop-org.apache.spark.deploy.master.Master-1-Master.out                        spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out.4
spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Master.out.2              spark-hadoop-org.apache.spark.deploy.master.Master-1-Master.out.1                      spark-hadoop-org.apache.spark.deploy.worker.Worker-1-Master.out
spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Master.out.3              spark-hadoop-org.apache.spark.deploy.master.Master-1-Master.out.2                      spark-hadoop-org.apache.spark.deploy.worker.Worker-1-Master.out.1
spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Master.out.4              spark-hadoop-org.apache.spark.deploy.master.Master-1-Master.out.3                      spark-hadoop-org.apache.spark.deploy.worker.Worker-1-Master.out.2
spark-hadoop-org.apache.spark.deploy.history.HistoryServer-1-Master.out.5              spark-hadoop-org.apache.spark.deploy.master.Master-1-Master.out.4                      spark-hadoop-org.apache.spark.deploy.worker.Worker-1-Master.out.3
spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out    spark-hadoop-org.apache.spark.deploy.master.Master-1-Master.out.5                      spark-hadoop-org.apache.spark.deploy.worker.Worker-1-Master.out.4
spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out.1  spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out    spark-hadoop-org.apache.spark.deploy.worker.Worker-1-Master.out.5
spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out.2  spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out.1
spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out.3  spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out.2
[hadoop@emr-header-1 spark-1.5.2]$ netstat  -apn | grep 8080
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:8080                0.0.0.0:*                   LISTEN      -                   
tcp        0      0 0.0.0.0:18080               0.0.0.0:*                   LISTEN      4650/java           
[hadoop@emr-header-1 spark-1.5.2]$ netstat  -apn | grep 8081
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:8081                0.0.0.0:*                   LISTEN      31639/java          
[hadoop@emr-header-1 spark-1.5.2]$ netstat  -apn | grep 8082
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
tcp        0      0 0.0.0.0:8082                0.0.0.0:*                   LISTEN      31864/java          
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-env.sh
[hadoop@emr-header-1 spark-1.5.2]$ cat /etc/hosts
127.0.0.1 localhost
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
10.46.79.84 iZ23vt9ykohZ
10.46.71.22 iZ23o7qodjnZ
10.25.6.165 iZ23j3z85poZ
10.25.113.77 iZ25h5j1pblZ
10.25.197.239 iZ2582bjjvsZ
10.25.233.107 iZ23fcl3kmtZ
10.25.84.144 iZ239jrw8q2Z
10.26.248.27 iZ23xqt8umoZ
10.28.108.176 iZbp1ielabd9elwzi6s9vjZ
# todo delete when meta service ready
1.1.1.1   emr.pre.ap-southeast-1.aliyuncs.com
1.1.1.1   emr.pre.cn-shenzhen.aliyuncs.com
1.1.1.1   emr.pre.cn-qingdao.aliyuncs.com
1.1.1.1   emr.pre.cn-beijing.aliyuncs.com
1.1.1.1   emr.pre.cn-shanghai.aliyuncs.com
1.1.1.1   emr.pre.cn-hongkong.aliyuncs.com
1.1.1.1   emr.pre.us-west-1.aliyuncs.com

1.1.1.1   emr-as.pre.ap-southeast-1.aliyuncs.com
1.1.1.1   emr-as.pre.cn-shenzhen.aliyuncs.com
1.1.1.1   emr-as.pre.cn-qingdao.aliyuncs.com
1.1.1.1   emr-as.pre.cn-beijing.aliyuncs.com
1.1.1.1   emr-as.pre.cn-shanghai.aliyuncs.com
1.1.1.1   emr-as.pre.cn-hongkong.aliyuncs.com
1.1.1.1   emr-as.pre.us-west-1.aliyuncs.com

1.1.1.1   emr.ap-southeast-1.aliyuncs.com
1.1.1.1   emr.cn-shenzhen.aliyuncs.com
1.1.1.1   emr.cn-qingdao.aliyuncs.com
1.1.1.1   emr.cn-beijing.aliyuncs.com
1.1.1.1   emr.cn-shanghai.aliyuncs.com
1.1.1.1   emr.cn-hongkong.aliyuncs.com
1.1.1.1   emr.us-west-1.aliyuncs.com

1.1.1.1   emr-as.ap-southeast-1.aliyuncs.com
1.1.1.1   emr-as.cn-shenzhen.aliyuncs.com
1.1.1.1   emr-as.cn-qingdao.aliyuncs.com
1.1.1.1   emr-as.cn-beijing.aliyuncs.com
1.1.1.1   emr-as.cn-shanghai.aliyuncs.com
1.1.1.1   emr-as.cn-hongkong.aliyuncs.com
1.1.1.1   emr-as.us-west-1.aliyuncs.com
#start add cluster host of cluster 36293,Fri Dec 30 15:46:01 CST 2016
10.27.71.108  emr-worker-2.cluster-36293 emr-worker-2 emr-header-3 iZbp108l83nak4aljhz5ltZ
10.27.71.161  emr-header-1.cluster-36293 emr-header-1 iZbp16l88xgquw1qzwm6yjZ
10.27.71.215  emr-worker-4.cluster-36293 emr-worker-4 iZbp1459u86is1csh19z1vZ
10.27.71.163  emr-worker-3.cluster-36293 emr-worker-3 iZbp11nfqrmmcjzsybllh5Z
10.27.71.100  emr-worker-1.cluster-36293 emr-worker-1 emr-header-2 iZbp1j9eycwskqr570uxwjZ
#end add cluster host

[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-env.sh
[hadoop@emr-header-1 spark-1.5.2]$ dispatch.sh conf/spark-env.sh
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
spark-env.sh                                                                                                                                                                                                                                100% 4054     4.0KB/s   00:00    
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ cd sbin/
[hadoop@emr-header-1 sbin]$ ls
derby.log            metastore_db     spark-daemon.sh   start-history-server.sh    start-mesos-shuffle-service.sh  start-slaves.sh        stop-history-server.sh    stop-mesos-shuffle-service.sh  stop-slaves.sh
hs_err_pid15797.log  slaves.sh        spark-daemons.sh  start-master.sh            start-shuffle-service.sh        start-thriftserver.sh  stop-master.sh            stop-shuffle-service.sh        stop-thriftserver.sh
hs_err_pid2696.log   spark-config.sh  start-all.sh      start-mesos-dispatcher.sh  start-slave.sh                  stop-all.sh            stop-mesos-dispatcher.sh  stop-slave.sh
[hadoop@emr-header-1 sbin]$ vi start-master.sh 
[hadoop@emr-header-1 sbin]$ dispatch.sh start-master.sh 
start-master.sh                                                                                                                                                                                                                             100% 1881     1.8KB/s   00:00    
start-master.sh                                                                                                                                                                                                                             100% 1881     1.8KB/s   00:00    
start-master.sh                                                                                                                                                                                                                             100% 1881     1.8KB/s   00:00    
start-master.sh                                                                                                                                                                                                                             100% 1881     1.8KB/s   00:00    
[hadoop@emr-header-1 sbin]$ cd ../
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ 
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/spark-env.sh
[hadoop@emr-header-1 spark-1.5.2]$ vi conf/slaves
[hadoop@emr-header-1 spark-1.5.2]$ dispatch.sh conf/slaves
slaves                                                                                                                                                                                                                                      100%  193     0.2KB/s   00:00    
slaves                                                                                                                                                                                                                                      100%  193     0.2KB/s   00:00    
slaves                                                                                                                                                                                                                                      100%  193     0.2KB/s   00:00    
slaves                                                                                                                                                                                                                                      100%  193     0.2KB/s   00:00    
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ vi run.sh
[hadoop@emr-header-1 spark-1.5.2]$ chmod 755 run.sh 
[hadoop@emr-header-1 spark-1.5.2]$ ./run.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  run.sh  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/s
slaves.sh                       spark-daemons.sh                start-master.sh                 start-shuffle-service.sh        start-thriftserver.sh           stop-master.sh                  stop-shuffle-service.sh         stop-thriftserver.sh
spark-config.sh                 start-all.sh                    start-mesos-dispatcher.sh       start-slave.sh                  stop-all.sh                     stop-mesos-dispatcher.sh        stop-slave.sh                   
spark-daemon.sh                 start-history-server.sh         start-mesos-shuffle-service.sh  start-slaves.sh                 stop-history-server.sh          stop-mesos-shuffle-service.sh   stop-slaves.sh                  
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/s
slaves.sh                       spark-daemons.sh                start-master.sh                 start-shuffle-service.sh        start-thriftserver.sh           stop-master.sh                  stop-shuffle-service.sh         stop-thriftserver.sh
spark-config.sh                 start-all.sh                    start-mesos-dispatcher.sh       start-slave.sh                  stop-all.sh                     stop-mesos-dispatcher.sh        stop-slave.sh                   
spark-daemon.sh                 start-history-server.sh         start-mesos-shuffle-service.sh  start-slaves.sh                 stop-history-server.sh          stop-mesos-shuffle-service.sh   stop-slaves.sh                  
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  run.sh  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ pwd
/home/hadoop/spark-1.5.2
[hadoop@emr-header-1 spark-1.5.2]$ vi /etc/profile
[hadoop@emr-header-1 spark-1.5.2]$ source /etc/profile
[hadoop@emr-header-1 spark-1.5.2]$ dispatch.sh /etc/profile
profile                                                                                                                                                                                                                                     100% 1947     1.9KB/s   00:00    
profile                                                                                                                                                                                                                                     100% 1947     1.9KB/s   00:00    
profile                                                                                                                                                                                                                                     100% 1947     1.9KB/s   00:00    
profile                                                                                                                                                                                                                                     100% 1947     1.9KB/s   00:00    
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/start-all.sh 
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ vi /etc/profile
[hadoop@emr-header-1 spark-1.5.2]$ source /etc/profile
[hadoop@emr-header-1 spark-1.5.2]$ dispatch.sh /etc/profile
profile                                                                                                                                                                                                                                     100% 1951     1.9KB/s   00:00    
profile                                                                                                                                                                                                                                     100% 1951     1.9KB/s   00:00    
profile                                                                                                                                                                                                                                     100% 1951     1.9KB/s   00:00    
profile                                                                                                                                                                                                                                     100% 1951     1.9KB/s   00:00    
[hadoop@emr-header-1 spark-1.5.2]$ ./run.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.master.Master-1-emr-header-1.cluster-36293.out
localhost: starting org.apache.spark.deploy.worker.Worker, logging to /home/hadoop/spark-1.5.2/sbin/../logs/spark-hadoop-org.apache.spark.deploy.worker.Worker-1-emr-header-1.cluster-36293.out
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  run.sh  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ ./sbin/stop-all.sh 
localhost: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
[hadoop@emr-header-1 spark-1.5.2]$ 
[hadoop@emr-header-1 spark-1.5.2]$ ls
bin  CHANGES.txt  conf  data  derby.log  ec2  examples  lib  LICENSE  licenses  logs  metastore_db  NOTICE  python  R  README.md  RELEASE  run.sh  sbin  work
[hadoop@emr-header-1 spark-1.5.2]$ cd
[hadoop@emr-header-1 ~]$ ls
aliyunDSW2atmtimequeryHDFSyarnclient.sh                       backup            DL8Line.fasta.gz  get.sh                  queryD                    SparkPi.jar   txtaliyunDSW2timequeryHDFSyarnclientD8yarn201612302211.txt      xubo20161230
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345tail.txt  bin               DL9Line.fasta     lib                     query.tar.gz              submitJob.sh  txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302214.txt      xubo201612311525.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345.txt      dispatch.sh       DL9Line.fasta.gz  ossforprestorestart.sh  realpath.sh               test          txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219tail.txt  xubo201612312030.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302523,txt      DL1Line.fasta     DSA.jar           putrun.sh               run.sh                    test_isa      txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219.txt
aliyunDSW2timequeryHDFS.sh                                    DL1Line.fasta.gz  getdataref.sh     put.sh                  spark-1.5.2               test.sh       v1.1.1.tar.gz
aliyunDSW2timequeryHDFSyarnclient.sh                          DL8Line.fasta     getdata.sh        query                   spark-1.5.2Master.tar.gz  tools         xubo
[hadoop@emr-header-1 ~]$ sz xubo20161231
xubo201612311525.tar.gz  xubo201612312030.tar.gz  
[hadoop@emr-header-1 ~]$ sz xubo20161231
xubo201612311525.tar.gz  xubo201612312030.tar.gz  
[hadoop@emr-header-1 ~]$ sz xubo201612312030.tar.gz 
[hadoop@emr-header-1 ~]$ 

[hadoop@emr-header-1 ~]$ ls
aliyunDSW2atmtimequeryHDFSyarnclient.sh                       backup            DL8Line.fasta.gz  get.sh                  queryD                    SparkPi.jar   txtaliyunDSW2timequeryHDFSyarnclientD8yarn201612302211.txt      xubo20161230
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345tail.txt  bin               DL9Line.fasta     lib                     query.tar.gz              submitJob.sh  txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302214.txt      xubo201612311525.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345.txt      dispatch.sh       DL9Line.fasta.gz  ossforprestorestart.sh  realpath.sh               test          txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219tail.txt  xubo201612312030.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302523,txt      DL1Line.fasta     DSA.jar           putrun.sh               run.sh                    test_isa      txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219.txt
aliyunDSW2timequeryHDFS.sh                                    DL1Line.fasta.gz  getdataref.sh     put.sh                  spark-1.5.2               test.sh       v1.1.1.tar.gz
aliyunDSW2timequeryHDFSyarnclient.sh                          DL8Line.fasta     getdata.sh        query                   spark-1.5.2Master.tar.gz  tools         xubo
[hadoop@emr-header-1 ~]$ rm -r xubo
[hadoop@emr-header-1 ~]$ hadoop fs -get /xubo .
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apps/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/apps/hbase-1.1.1/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/apps/tez-0.8.4/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[hadoop@emr-header-1 ~]$ hadoop fs -mkdir /backup
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apps/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/apps/hbase-1.1.1/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/apps/tez-0.8.4/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[hadoop@emr-header-1 ~]$ hadoop fs -mv /xubo /backup/xubo5time201612312141
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/opt/apps/hadoop-2.7.2/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/apps/hbase-1.1.1/lib/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/apps/tez-0.8.4/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
[hadoop@emr-header-1 ~]$ rm DSA.jar 
[hadoop@emr-header-1 ~]$ rz
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201612312143D9L392N100.txt 
BLOSUM62	/queryD/D9L392N100	/Luniref/DL9Line.fasta	128	1	5	11	1	
[Stage 1:======>                                                (16 + 16) / 128]Exception in thread "main" org.apache.spark.SparkException: Job 1 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)
	at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1740)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1739)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$.main(aliyunDSW2ATMQueryTime.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
^C
[hadoop@emr-header-1 ~]$ ls
1.txt                                                               aliyunDSW2timequeryHDFSyarnclient.sh  DL9Line.fasta           putrun.sh     spark-1.5.2Master.tar.gz                                    txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302214.txt
aliyunDSW2atmtimequeryHDFSyarnclient.sh                             backup                                DL9Line.fasta.gz        put.sh        SparkPi.jar                                                 txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219tail.txt
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345tail.txt        bin                                   DSA.jar                 query         submitJob.sh                                                txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219.txt
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345.txt            dispatch.sh                           getdataref.sh           queryD        test                                                        v1.1.1.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302523,txt            DL1Line.fasta                         getdata.sh              query.tar.gz  test_isa                                                    xubo
aliyunDSW2atmtimequeryHDFSyarnclienttime201612312139D91024N1.txt    DL1Line.fasta.gz                      get.sh                  realpath.sh   test.sh                                                     xubo20161230
aliyunDSW2atmtimequeryHDFSyarnclienttime201612312143D9L392N100.txt  DL8Line.fasta                         lib                     run.sh        tools                                                       xubo201612311525.tar.gz
aliyunDSW2timequeryHDFS.sh                                          DL8Line.fasta.gz                      ossforprestorestart.sh  spark-1.5.2   txtaliyunDSW2timequeryHDFSyarnclientD8yarn201612302211.txt  xubo201612312030.tar.gz
[hadoop@emr-header-1 ~]$ tar -zcvf xubo201612312154.tar.gz xubo
xubo/
xubo/project/
xubo/project/SparkSW/
xubo/project/SparkSW/output/
xubo/project/SparkSW/output/time/
xubo/project/SparkSW/output/time/20161231204903135DSW2ATM_queryFile_D9L10240N1_dbFile_DL9Line.fasta_splitNum_128_taskNum_1_topK_5/
xubo/project/SparkSW/output/time/20161231204903135DSW2ATM_queryFile_D9L10240N1_dbFile_DL9Line.fasta_splitNum_128_taskNum_1_topK_5/part-00000
xubo/project/SparkSW/output/time/20161231204903135DSW2ATM_queryFile_D9L10240N1_dbFile_DL9Line.fasta_splitNum_128_taskNum_1_topK_5/_SUCCESS
xubo/project/SparkSW/output/time/20161231211355445DSW2ATM_queryFile_D9L10240N1_dbFile_DL9Line.fasta_splitNum_128_taskNum_1_topK_5/
xubo/project/SparkSW/output/time/20161231211355445DSW2ATM_queryFile_D9L10240N1_dbFile_DL9Line.fasta_splitNum_128_taskNum_1_topK_5/part-00000
xubo/project/SparkSW/output/time/20161231211355445DSW2ATM_queryFile_D9L10240N1_dbFile_DL9Line.fasta_splitNum_128_taskNum_1_topK_5/_SUCCESS
[hadoop@emr-header-1 ~]$ sz xubo201612312
xubo201612312030.tar.gz  xubo201612312154.tar.gz  
[hadoop@emr-header-1 ~]$ sz xubo201612312
xubo201612312030.tar.gz  xubo201612312154.tar.gz  
[hadoop@emr-header-1 ~]$ sz xubo201612312154.tar.gz 
[hadoop@emr-header-1 ~]$ 

[hadoop@emr-header-1 ~]$ 
[hadoop@emr-header-1 ~]$ ls
1.txt                                                               backup            DSA.jar                 queryD                    test_isa                                                        xubo20161230
aliyunDSW2atmtimequeryHDFSyarnclient.sh                             bin               getdataref.sh           query.tar.gz              test.sh                                                         xubo201612311525.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345tail.txt        dispatch.sh       getdata.sh              realpath.sh               tools                                                           xubo201612312030.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302345.txt            DL1Line.fasta     get.sh                  run.sh                    txtaliyunDSW2timequeryHDFSyarnclientD8yarn201612302211.txt      xubo201612312154.tar.gz
aliyunDSW2atmtimequeryHDFSyarnclienttime201612302523,txt            DL1Line.fasta.gz  lib                     spark-1.5.2               txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302214.txt
aliyunDSW2atmtimequeryHDFSyarnclienttime201612312139D91024N1.txt    DL8Line.fasta     ossforprestorestart.sh  spark-1.5.2Master.tar.gz  txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219tail.txt
aliyunDSW2atmtimequeryHDFSyarnclienttime201612312143D9L392N100.txt  DL8Line.fasta.gz  putrun.sh               SparkPi.jar               txtaliyunDSW2timequeryHDFSyarnclientD9yarn201612302219.txt
aliyunDSW2timequeryHDFS.sh                                          DL9Line.fasta     put.sh                  submitJob.sh              v1.1.1.tar.gz
aliyunDSW2timequeryHDFSyarnclient.sh                                DL9Line.fasta.gz  query                   test                      xubo
[hadoop@emr-header-1 ~]$ sz xubo201612312030.tar.gz 

[hadoop@emr-header-1 ~]$ free -m
             total       used       free     shared    buffers     cached
Mem:         15951      12718       3233          0        310       6943
-/+ buffers/cache:       5463      10487
Swap:            0          0          0
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40.txt 
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
^C
[hadoop@emr-header-1 ~]$ cat aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40.txt 
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	128	1	5	11	1	
Exception in thread "main" java.lang.IllegalArgumentException: Required executor memory (15360+1536 MB) is above the max threshold (13088 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.
	at org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:283)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:139)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:530)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:167)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$.main(aliyunDSW2ATMQueryTime.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40M12G.txt 
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
^C
[hadoop@emr-header-1 ~]$ cat aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40M12G.txt 
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	128	1	5	11	1	
Exception in thread "main" java.lang.IllegalArgumentException: Required executor memory (12288+1228 MB) is above the max threshold (13088 MB) of this cluster! Please check the values of 'yarn.scheduler.maximum-allocation-mb' and/or 'yarn.nodemanager.resource.memory-mb'.
	at org.apache.spark.deploy.yarn.Client.verifyClusterResources(Client.scala:283)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:139)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:530)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:167)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$.main(aliyunDSW2ATMQueryTime.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201612312212D9L392N40M10G.txt 
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	128	1	5	11	1	
[Stage 2:>                                                       (0 + 16) / 128]^C
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201612312218D9L392N40M10G.txt 
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	128	1	5	11	1	
[Stage 19:=====================================================>(126 + 2) / 128]Exception in thread "main" org.apache.spark.SparkException: Job 19 cancelled because SparkContext was shut down
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:806)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:804)
	at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)
	at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:804)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:1658)
	at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1581)
	at org.apache.spark.SparkContext$$anonfun$stop$9.apply$mcV$sp(SparkContext.scala:1740)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1739)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1952)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1025)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:1007)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1397)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1384)
	at org.apache.spark.rdd.RDD$$anonfun$top$1.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.top(RDD.scala:1364)
	at org.dsa.core.DSW2ATM.align(DSW2ATM.scala:53)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:43)
	at org.dsa.core.DSASequenceAlignment$$anonfun$run$1.apply(DSASequenceAlignment.scala:42)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.dsa.core.DSASequenceAlignment.run(DSASequenceAlignment.scala:42)
	at org.dsa.core.DSW2ATM$.main(DSW2ATM.scala:169)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:19)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$$anonfun$main$1.apply$mcVI$sp(aliyunDSW2ATMQueryTime.scala:14)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
	at org.dsa.time.aliyunDSW2ATMQueryTime$.main(aliyunDSW2ATMQueryTime.scala:13)
	at org.dsa.time.aliyunDSW2ATMQueryTime.main(aliyunDSW2ATMQueryTime.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
^C
[hadoop@emr-header-1 ~]$ tail -f aliyunDSW2atmtimequeryHDFSyarnclienttime201612312243D9L392N40M10G.txt 
BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	256	1	5	11	1	
topK:5 Query:UniRef100_S2JIB5                                                   
AlignmentRecord(UniRef100_S2JIB5, 392M, 2136, 1098, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A168Q3X9, 169M1D188M1D35M, 2027, 1063, 0, 393, 0, 391, 196)
AlignmentRecord(UniRef100_A0A0C9M6E2, 169M1D191M3D32M, 1984, 1061, 0, 395, 0, 391, 198)
AlignmentRecord(UniRef100_A0A0B7NSV9, 22M1D2M2D45M12I66M2I24M2D5M7D23M4I11M1I11M19I22M9I17M1I35M2D27M4I7M1D11M, 1018, 619, 0, 342, 0, 379, 140)
AlignmentRecord(UniRef100_A0A0C7BKV7, 3M1D19M1I134M10I7M5I37M27I18M10I30M4D18M1D10M1D26M4I7M5I20M, 863, 610, 0, 338, 0, 390, 141)

topK:5 Query:UniRef100_A0A177A2S8
AlignmentRecord(UniRef100_A0A177A2S8, 392M, 2078, 1035, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A177A9G5, 8M3D2M8D244M, 1272, 249, 138, 310, 138, 391, 113)
AlignmentRecord(UniRef100_L8FXF9, 171M, 894, 0, 221, 170, 221, 391, 0)
AlignmentRecord(UniRef100_A0A177A0Y6, 2M2D29M7D37M5I10M1D11M1I140M12I21M2I16M1D16M9I21M3I47M, 790, 337, 0, 360, 0, 381, 163)
AlignmentRecord(UniRef100_A0A177A2V3, 20M1I9M4I22M1I8M2I12M1D3M3I47M30I85M1D23M2I12M1I11M10I26M16D46M, 690, 255, 5, 348, 5, 382, 151)

topK:5 Query:UniRef100_A0A0F4GJW2
AlignmentRecord(UniRef100_A0A0F4GJW2, 392M, 1982, 974, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_F9XNJ0, 392M, 1952, 956, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A139H680, 264M1D84M3D44M, 1214, 673, 0, 395, 0, 391, 198)
AlignmentRecord(UniRef100_A0A139H4Y7, 264M1D85M3D43M, 1191, 653, 0, 395, 0, 391, 198)
AlignmentRecord(UniRef100_M3AH46, 264M1D84M3D44M, 1185, 646, 0, 395, 0, 391, 198)

topK:5 Query:UniRef100_A0A0F4GK02
AlignmentRecord(UniRef100_A0A0F4GK02, 392M, 2023, 1012, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_F9WXN5, 392M, 1951, 982, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_M3C188, 107M1D55M3D3M1I184M3I18M3I18M, 1362, 638, 0, 388, 0, 391, 191)
AlignmentRecord(UniRef100_Q0UA54, 59M2D48M1D55M1D186M3I17M5I18M, 1348, 603, 0, 386, 0, 390, 189)
AlignmentRecord(UniRef100_A0A139H217, 107M1D58M2D72M1D113M3I19M5I15M, 1303, 632, 0, 541, 0, 391, 344)

topK:5 Query:UniRef100_A0A0F4GFF4
AlignmentRecord(UniRef100_A0A0F4GFF4, 392M, 2147, 1045, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_F9X805, 376M, 1976, 903, 15, 377, 15, 390, 180)
AlignmentRecord(UniRef100_M3A189, 209M1I166M, 1701, 705, 15, 376, 15, 390, 179)
AlignmentRecord(UniRef100_A0A139H354, 209M1I166M, 1660, 684, 15, 388, 15, 390, 191)
AlignmentRecord(UniRef100_A0A139IN61, 209M1I53M6I107M, 1606, 635, 15, 370, 15, 390, 173)

topK:5 Query:UniRef100_A0A0I9XEW1
AlignmentRecord(UniRef100_A0A0I9XEW1, 392M, 2050, 986, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W7MRX3, 391M, 1969, 935, 0, 390, 0, 390, 193)
AlignmentRecord(UniRef100_X0JM95, 11M1I41M1I303M1D32M, 1746, 805, 0, 387, 0, 388, 190)
AlignmentRecord(UniRef100_W9JLI0, 11M1I41M1I86M16I201M1D32M, 1649, 700, 0, 371, 0, 388, 174)
AlignmentRecord(UniRef100_X0LHU8, 45M1I124M19I195M, 1629, 740, 8, 387, 8, 391, 190)

topK:5 Query:UniRef100_S2K2M3
AlignmentRecord(UniRef100_S2K2M3, 392M, 2024, 993, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A168IXM9, 371M1D9M, 1777, 1592, 0, 380, 0, 379, 576)
AlignmentRecord(UniRef100_A0A0C9MNM2, 392M, 1758, 916, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0B7N954, 359M, 1577, 742, 0, 358, 0, 358, 161)
AlignmentRecord(UniRef100_I1C7D7, 174M1I40M2D106M1D68M, 1052, 575, 2, 394, 2, 390, 192)

topK:5 Query:UniRef100_A0A0I9Y7X6
AlignmentRecord(UniRef100_A0A0I9Y7X6, 392M, 2110, 1058, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_S0DXB3, 392M, 2105, 1053, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9J8S2, 309M2D83M, 2035, 1027, 0, 425, 0, 391, 228)
AlignmentRecord(UniRef100_W9KZ87, 309M2D83M, 2030, 1024, 0, 425, 0, 391, 228)
AlignmentRecord(UniRef100_X0LCY9, 309M2D83M, 2026, 1023, 0, 425, 0, 391, 228)

topK:5 Query:UniRef100_A0A0F4GX45
AlignmentRecord(UniRef100_A0A0F4GX45, 392M, 2136, 1061, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_F9X6B3, 392M, 2094, 1057, 0, 461, 0, 391, 264)
AlignmentRecord(UniRef100_A0A139HJU6, 364M1I4M1I22M, 1918, 1006, 0, 459, 0, 391, 262)
AlignmentRecord(UniRef100_M2ZXX0, 364M1I4M1I22M, 1909, 1008, 0, 459, 0, 391, 262)
AlignmentRecord(UniRef100_N1PR65, 349M2I41M, 1894, 995, 0, 459, 0, 391, 262)

topK:5 Query:UniRef100_S2JR65
AlignmentRecord(UniRef100_S2JR65, 392M, 2202, 1145, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A168N1A8, 19M1I55M3D8M6D11M1I5M2D7M1D285M, 2009, 1010, 0, 401, 0, 391, 204)
AlignmentRecord(UniRef100_A0A0C9LTQ6, 24M1I50M3D17M2I6M1D7M1D251M, 1835, 845, 0, 359, 0, 357, 162)
AlignmentRecord(UniRef100_A0A0B7N0C1, 60M4D272M3D12M10D48M, 1814, 1012, 0, 408, 0, 391, 210)
AlignmentRecord(UniRef100_A0A0C7BII8, 15M1D6M2D3M1I6M2D8M1I7M1D14M1I12M1D6M2I4M1I6M3I4M1I10M2I230M5I6M1D33M, 1460, 684, 0, 377, 0, 386, 180)

topK:5 Query:UniRef100_L2GNE6
AlignmentRecord(UniRef100_L2GNE6, 392M, 2106, 1075, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0M3K2W8, 22M16D7M3D27M2D13M3D80M5D36M, 118, 62, 95, 1121, 95, 279, 888)
AlignmentRecord(UniRef100_I6RZH7, 32M2D15M4I6M1I5M1I14M3I4M2I33M2D39M6I8M1D9M, 112, 24, 81, 257, 81, 262, 36)
AlignmentRecord(UniRef100_A0A198FSL0, 19M5I28M1D22M2I37M2I8M, 112, 0, 66, 188, 66, 188, 0)
AlignmentRecord(UniRef100_J9I631, 18M1I7M2I27M2D19M2D17M1D17M1D48M, 102, 47, 124, 334, 124, 279, 802)

topK:5 Query:UniRef100_A0A141PZB2
AlignmentRecord(UniRef100_A0A141PZB2, 392M, 2137, 1057, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_Q0CWW4, 392M, 2101, 1895, 0, 788, 0, 391, 984)
AlignmentRecord(UniRef100_A2QYC2, 392M, 2093, 1887, 0, 788, 0, 391, 984)
AlignmentRecord(UniRef100_A0A117E303, 392M, 2090, 1884, 0, 788, 0, 391, 984)
AlignmentRecord(UniRef100_A1CG49, 392M, 2083, 1877, 0, 789, 0, 391, 985)

topK:5 Query:UniRef100_A0A176ZZT9
AlignmentRecord(UniRef100_A0A176ZZT9, 392M, 2107, 1044, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_L8FQ44, 245M15D147M, 2082, 1121, 0, 406, 0, 391, 209)
AlignmentRecord(UniRef100_A0A094HDT4, 245M15D129M, 1748, 1565, 0, 388, 0, 373, 587)
AlignmentRecord(UniRef100_A0A094EID0, 71M1I173M15D129M, 1740, 1557, 0, 387, 0, 373, 586)
AlignmentRecord(UniRef100_A0A094HSD8, 108M1D9M1D41M1D87M15D129M, 1678, 1495, 0, 391, 0, 373, 590)

topK:5 Query:UniRef100_A0A0F4GHI1
AlignmentRecord(UniRef100_A0A0F4GHI1, 392M, 2084, 1030, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_N1Q2Y9, 190M14D196M, 1688, 839, 6, 406, 6, 391, 196)
AlignmentRecord(UniRef100_N1QJS8, 194M2D12M9D6M1D178M, 1678, 860, 2, 401, 2, 391, 193)
AlignmentRecord(UniRef100_M2N5N6, 119M1D73M16D195M, 1633, 816, 5, 413, 5, 391, 214)
AlignmentRecord(UniRef100_A0A139HXV6, 196M5D13M1D178M, 1620, 839, 5, 398, 5, 391, 197)

topK:5 Query:UniRef100_A0A0I9Y9K1
AlignmentRecord(UniRef100_A0A0I9Y9K1, 392M, 2064, 997, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_S0EJ06, 392M, 2029, 967, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9JUC6, 392M, 1885, 909, 0, 391, 0, 391, 193)
AlignmentRecord(UniRef100_W9HR16, 392M, 1843, 890, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_X0JSP0, 66M8I318M, 1818, 848, 0, 383, 0, 391, 186)

topK:5 Query:UniRef100_A0A0J0CJ97
AlignmentRecord(UniRef100_A0A0J0CJ97, 392M, 2157, 1094, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_S0DUF2, 392M, 2149, 1090, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_F9FTG3, 184M8D208M, 2119, 1060, 0, 399, 0, 391, 202)
AlignmentRecord(UniRef100_N4TZ35, 184M34D208M, 2093, 1034, 0, 425, 0, 391, 228)
AlignmentRecord(UniRef100_A0A0D2XDQ1, 184M32D208M, 2092, 1036, 0, 423, 0, 391, 226)

topK:5 Query:UniRef100_C5M453
AlignmentRecord(UniRef100_C5M453, 392M, 2105, 1056, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_B9WL62, 237M2D149M4D6M, 1620, 866, 0, 397, 0, 391, 200)
AlignmentRecord(UniRef100_A0A0A6JYV9, 237M2D149M4D6M, 1604, 864, 0, 397, 0, 391, 200)
AlignmentRecord(UniRef100_Q59LU6, 237M2D149M4D6M, 1604, 864, 0, 397, 0, 391, 200)
AlignmentRecord(UniRef100_M3IQP4, 237M2D148M, 1587, 817, 0, 386, 0, 384, 189)

topK:5 Query:UniRef100_M7UDA0
AlignmentRecord(UniRef100_M7UDA0, 392M, 2078, 1045, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_G2YAA7, 90M2D302M, 2066, 1033, 0, 393, 0, 391, 196)
AlignmentRecord(UniRef100_W9CMZ4, 392M, 2020, 1000, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A7ES72, 392M, 2006, 1005, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A194X647, 14M5D20M7I351M, 1821, 822, 0, 389, 0, 391, 192)

topK:5 Query:UniRef100_G2YLV2
AlignmentRecord(UniRef100_G2YLV2, 392M, 2063, 1028, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9C2C1, 20M1I8M2I69M1I245M, 1564, 594, 46, 341, 46, 391, 144)
AlignmentRecord(UniRef100_A7ESR1, 60M1I245M, 1391, 451, 86, 304, 86, 391, 107)
AlignmentRecord(UniRef100_A0A194X7Y6, 78M2D140M1I98M, 986, 358, 75, 330, 75, 391, 132)
AlignmentRecord(UniRef100_S3CS45, 7M5I10M15I3M1D46M3D12M5D10M1D138M4D9M4D83M2I6M, 939, 349, 46, 341, 46, 391, 144)

topK:5 Query:UniRef100_G2XUL9
AlignmentRecord(UniRef100_G2XUL9, 392M, 2108, 1039, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A7E6K0, 392M, 2094, 1039, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9CCG7, 392M, 2079, 1034, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_K1WVI0, 388M, 1982, 990, 0, 387, 0, 387, 190)
AlignmentRecord(UniRef100_A0A0C3D2X0, 392M, 1975, 996, 0, 391, 0, 391, 194)

topK:5 Query:UniRef100_Q4X0L2
AlignmentRecord(UniRef100_Q4X0L2, 392M, 2150, 1041, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0J5PYL8, 319M, 1767, 658, 73, 335, 73, 391, 138)
AlignmentRecord(UniRef100_A0A0S7DUI3, 319M, 1752, 648, 73, 335, 73, 391, 138)
AlignmentRecord(UniRef100_A1DHL4, 319M, 1751, 654, 73, 335, 73, 391, 138)
AlignmentRecord(UniRef100_A0A0K8LG67, 319M, 1733, 640, 73, 335, 73, 391, 138)

topK:5 Query:UniRef100_W9Z3K9
AlignmentRecord(UniRef100_W9Z3K9, 392M, 2110, 1041, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9YAN3, 392M, 1743, 899, 0, 393, 0, 391, 196)
AlignmentRecord(UniRef100_H6BNH0, 392M, 1699, 890, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0D2JC67, 205M1I186M, 1615, 871, 0, 390, 0, 391, 193)
AlignmentRecord(UniRef100_A0A0D2DGG9, 194M4I194M, 1547, 809, 0, 387, 0, 391, 190)

topK:5 Query:UniRef100_C5M6U4
AlignmentRecord(UniRef100_C5M6U4, 392M, 2102, 1040, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_C4YIA7, 20M1I369M, 1796, 873, 2, 392, 2, 391, 195)
AlignmentRecord(UniRef100_Q5AHJ9, 20M1I369M, 1793, 870, 2, 392, 2, 391, 195)
AlignmentRecord(UniRef100_A0A0A6KWM1, 17M1I372M, 1792, 866, 2, 392, 2, 391, 195)
AlignmentRecord(UniRef100_B9WB98, 18M1I369M, 1788, 870, 4, 392, 4, 391, 195)

topK:5 Query:UniRef100_G2Y5H4
AlignmentRecord(UniRef100_G2Y5H4, 392M, 2052, 1037, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A7EJV3, 392M, 1990, 1020, 0, 928, 0, 391, 731)
AlignmentRecord(UniRef100_W9C6J0, 391M, 1954, 1006, 0, 926, 0, 390, 729)
AlignmentRecord(UniRef100_A0A194XTT2, 392M, 1888, 973, 0, 931, 0, 391, 734)
AlignmentRecord(UniRef100_K1WB96, 392M, 1873, 986, 0, 931, 0, 391, 734)

topK:5 Query:UniRef100_Q4WRX7
AlignmentRecord(UniRef100_Q4WRX7, 392M, 2192, 1111, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0J5PH90, 392M, 2188, 1110, 0, 436, 0, 391, 239)
AlignmentRecord(UniRef100_A1D1U2, 6M5D386M, 2121, 1091, 0, 396, 0, 391, 199)
AlignmentRecord(UniRef100_A0A0S7E2L3, 392M, 2117, 1095, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0K8LNH0, 90M2I102M1I197M, 2062, 1055, 0, 388, 0, 391, 191)

topK:5 Query:UniRef100_G2YHU2
AlignmentRecord(UniRef100_G2YHU2, 392M, 2114, 1078, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9CJA4, 258M2I132M, 1946, 1009, 0, 389, 0, 391, 192)
AlignmentRecord(UniRef100_A7E422, 113M37I111M6D82M1D48M, 1638, 768, 0, 360, 0, 390, 163)
AlignmentRecord(UniRef100_A0A194XNL1, 19M2D218M6D83M1D49M, 1416, 708, 23, 395, 23, 391, 197)
AlignmentRecord(UniRef100_A0A0C3H9D6, 22M3I15M2D160M1D63M5D7M3I8M4D61M1D47M, 1374, 729, 2, 396, 2, 390, 199)

topK:5 Query:UniRef100_C5M8Q0
AlignmentRecord(UniRef100_C5M8Q0, 392M, 2066, 1006, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_M3K5S0, 29M1D6M1D356M, 1696, 768, 0, 392, 0, 390, 195)
AlignmentRecord(UniRef100_B9WIY6, 29M7D7M1I141M3D10M5D204M, 1685, 738, 0, 405, 0, 391, 208)
AlignmentRecord(UniRef100_C4YSM3, 29M5D7M1I144M2D211M, 1680, 726, 0, 397, 0, 391, 200)
AlignmentRecord(UniRef100_A0A0A6KAL2, 29M5D7M1I144M2D211M, 1680, 726, 0, 397, 0, 391, 200)

topK:5 Query:UniRef100_W9Z313
AlignmentRecord(UniRef100_W9Z313, 392M, 2128, 1065, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0D2IUZ8, 16M3D3M2I47M1I323M, 1638, 688, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_H6BJU9, 195M1D197M, 1620, 655, 0, 392, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0D2I6K5, 67M2I127M1D196M, 1610, 690, 0, 390, 0, 391, 193)
AlignmentRecord(UniRef100_A0A178CQ81, 46M1D21M2I127M1D196M, 1604, 679, 0, 391, 0, 391, 194)

topK:5 Query:UniRef100_G2YZ18
AlignmentRecord(UniRef100_G2YZ18, 392M, 2098, 1039, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A7EVN5, 392M, 2003, 995, 0, 499, 0, 391, 302)
AlignmentRecord(UniRef100_W9CBM2, 2M1D214M1D11M1D165M, 1963, 986, 0, 464, 0, 391, 267)
AlignmentRecord(UniRef100_A0A194X2X0, 213M1I169M, 1765, 877, 1, 388, 1, 383, 191)
AlignmentRecord(UniRef100_S3DE49, 211M4I110M2I11M4I47M, 1644, 827, 1, 449, 1, 389, 252)

topK:5 Query:UniRef100_W9XNF5
AlignmentRecord(UniRef100_W9XNF5, 392M, 2092, 1045, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0D2BGL5, 24M2I145M1I97M9D100M3I4M2D8M2I6M, 1601, 897, 0, 394, 0, 391, 197)
AlignmentRecord(UniRef100_A0A0D2FYH9, 24M2I144M1I97M7D95M4I15M, 1590, 824, 0, 381, 0, 381, 184)
AlignmentRecord(UniRef100_W9XLP5, 21M2I148M1I86M4D12M4D93M1I8M3I11M, 1589, 832, 0, 386, 0, 385, 188)
AlignmentRecord(UniRef100_A0A0D2CAA0, 24M2I144M1I97M7D116M2I6M, 1578, 876, 0, 393, 0, 391, 196)

topK:5 Query:UniRef100_W9XNZ5
AlignmentRecord(UniRef100_W9XNZ5, 392M, 2103, 1021, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0D2ILX7, 68M3I316M, 1616, 681, 0, 383, 0, 386, 182)
AlignmentRecord(UniRef100_W9XPD7, 52M1I319M, 1608, 679, 18, 371, 18, 389, 174)
AlignmentRecord(UniRef100_A0A0D2KIW7, 70M1I313M, 1542, 691, 0, 382, 0, 383, 185)
AlignmentRecord(UniRef100_W9WLX8, 70M1I313M, 1539, 664, 0, 382, 0, 383, 185)

topK:5 Query:UniRef100_Q4WVP7
AlignmentRecord(UniRef100_Q4WVP7, 392M, 2154, 1082, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0J5Q1D5, 392M, 2129, 1068, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A1DDQ0, 365M2I8M1I16M, 1946, 982, 0, 388, 0, 391, 191)
AlignmentRecord(UniRef100_A0A0S7E3N9, 365M2I25M, 1932, 957, 0, 389, 0, 391, 192)
AlignmentRecord(UniRef100_A0A0K8L3D6, 365M2I25M, 1892, 934, 0, 389, 0, 391, 192)

topK:5 Query:UniRef100_M7U8Z4
AlignmentRecord(UniRef100_M7U8Z4, 392M, 2182, 1072, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9CTF1, 66M4D48M1D106M1I2M1I70M24I8M8I27M4D6M9D4M1D17M, 1158, 585, 0, 372, 0, 387, 175)
AlignmentRecord(UniRef100_A7EU11, 182M1I8M15I8M4I60M, 868, 264, 111, 257, 111, 388, 60)
AlignmentRecord(UniRef100_S3DWW7, 3M2D17M6I23M6I14M4D8M5D16M1D12M1D17M3D79M9D4M4D7M11I4M2I13M2D5M1D7M3I15M, 303, 80, 1, 287, 1, 272, 83)
AlignmentRecord(UniRef100_S3D309, 16M4I22M6D7M1I6M2D3M2D17M9I91M7D4M9D13M5D9M1D6M2D4M5I39M, 277, 107, 29, 364, 29, 284, 161)

topK:5 Query:UniRef100_Q4WGF5
AlignmentRecord(UniRef100_Q4WGF5, 392M, 2085, 1020, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A1DCE8, 20M1D372M, 2035, 1008, 0, 392, 0, 391, 195)
AlignmentRecord(UniRef100_A0A0S7DM69, 20M1D372M, 2025, 1008, 0, 392, 0, 391, 195)
AlignmentRecord(UniRef100_A0A0K8LE33, 20M1D368M, 1967, 961, 0, 388, 0, 387, 191)
AlignmentRecord(UniRef100_A1CDH1, 12M1D8M1D279M1I92M, 1876, 959, 0, 392, 0, 391, 195)

topK:5 Query:UniRef100_W9Z1E5
AlignmentRecord(UniRef100_W9Z1E5, 392M, 2133, 1056, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_W9Z3P9, 234M5I68M3D28M8I18M1D20M4I4M, 1714, 929, 0, 375, 0, 388, 178)
AlignmentRecord(UniRef100_H6C2K6, 228M4I79M5D30M16I23M, 1587, 843, 0, 557, 0, 379, 360)
AlignmentRecord(UniRef100_A0A0D2IJJ8, 234M5I69M2D9M1D13M, 1466, 631, 0, 496, 0, 329, 299)
AlignmentRecord(UniRef100_A0A0D1ZAA7, 225M4I77M13D5M1I19M15I14M6D19M, 1457, 875, 0, 552, 0, 378, 355)

topK:5 Query:UniRef100_B2ADM6
AlignmentRecord(UniRef100_B2ADM6, 392M, 2133, 1045, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A090CPK0, 68M18D131M19I174M, 1955, 920, 0, 390, 0, 391, 193)
AlignmentRecord(UniRef100_A0A175W5G0, 68M29D61M20I48M19I15M1D60M1I22M3D4M11D15M, 749, 312, 4, 341, 4, 336, 144)
AlignmentRecord(UniRef100_A0A194VC52, 49M8D16M1D75M1D58M19I19M1I31M7I13M2I19M1D10M9I8M2D53M, 557, 393, 1, 365, 1, 389, 562)
AlignmentRecord(UniRef100_A0A179I7A2, 62M4I4M7D74M1D53M19I20M1I31M1D4M, 527, 132, 1, 258, 1, 272, 61)

topK:5 Query:UniRef100_C5MD91
AlignmentRecord(UniRef100_C5MD91, 392M, 2141, 1050, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_B9W8E1, 101M1D180M3D37M2D5M7D69M, 1378, 762, 0, 404, 0, 391, 206)
AlignmentRecord(UniRef100_A0A0A6L2Y7, 105M1D176M1D31M2I4M3D5M7D69M, 1359, 759, 0, 401, 0, 391, 202)
AlignmentRecord(UniRef100_Q59PV5, 105M1D176M1D31M2I4M3D5M7D69M, 1360, 759, 0, 401, 0, 391, 202)
AlignmentRecord(UniRef100_Q59PX9, 105M1D176M1D33M3D5M1D4M4D69M, 1351, 753, 0, 401, 0, 391, 202)

topK:5 Query:UniRef100_Q4WWZ5
AlignmentRecord(UniRef100_Q4WWZ5, 392M, 2122, 1038, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_B0XYM7, 392M, 2121, 1038, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A1D7T5, 392M, 2092, 1026, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0S7DLB1, 392M, 2070, 1001, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A0A0K8LD69, 392M, 2034, 993, 0, 391, 0, 391, 194)

topK:5 Query:UniRef100_M7UCQ6
AlignmentRecord(UniRef100_M7UCQ6, 392M, 2283, 1140, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_G2XZB1, 392M, 2279, 1136, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A7EFR4, 47M17I79M11I93M38I107M, 1687, 706, 0, 325, 0, 391, 125)
AlignmentRecord(UniRef100_W9CTS4, 23M2D13M2I9M49I47M11I79M8I2M26I123M, 1414, 415, 0, 297, 0, 391, 93)
AlignmentRecord(UniRef100_A0A179HBS9, 32M7D41M1D8M1I19M4I15M3I13M2D14M1I3M17I19M10I11M2D14M4D15M1I11M1D4M12D24M8I10M3I25M, 635, 328, 6, 308, 6, 331, 109)

topK:5 Query:UniRef100_M7V2E2
AlignmentRecord(UniRef100_M7V2E2, 392M, 2136, 1065, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_G2YSU6, 392M, 2123, 1060, 0, 391, 0, 391, 194)
AlignmentRecord(UniRef100_A7F7X6, 364M, 1852, 844, 28, 363, 28, 391, 166)
AlignmentRecord(UniRef100_W9CFJ0, 86M29I277M, 1730, 741, 0, 362, 0, 391, 165)
AlignmentRecord(UniRef100_A0A0C3HYV1, 135M3I218M1D8M, 1449, 594, 28, 361, 28, 391, 164)

BLOSUM62	/queryD/D9L392N40	/Luniref/DL9Line.fasta	256	1	5	11	1	
[Stage 4:>                                                       (0 + 16) / 256]

